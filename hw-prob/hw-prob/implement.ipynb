{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42fd52d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Determine most similar words in terms of their word embeddings.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from integerize import Integerizer   # look at integerize.py for more info\n",
    "\n",
    "# Needed for Python's optional type annotations.\n",
    "# We've included type annotations and recommend that you do the same, \n",
    "# so that mypy (or a similar package) can catch type errors in your code.\n",
    "from typing import List, Optional\n",
    "\n",
    "try:\n",
    "    # PyTorch is your friend. Not using it will make your program so slow.\n",
    "    # And it's also required for this assignment. ;-)\n",
    "    # So if you comment this block out instead of dealing with it, you're\n",
    "    # making your own life worse.\n",
    "    import torch    \n",
    "    import torch.nn as nn\n",
    "except ImportError:\n",
    "    print(\"\\nERROR! You need to install Miniconda, then create and activate the nlp-class environment.  See the INSTRUCTIONS file.\\n\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# log = logging.getLogger(Path(__file__).stem)  # The only okay global variable.\n",
    "\n",
    "# Logging is in general a good practice to monitor the behavior of your code\n",
    "# while it's running. Compared to calling `print`, it provides two benefits.\n",
    "# \n",
    "# - It prints to standard error (stderr), not standard output (stdout) by\n",
    "#   default.  So these messages will normally go to your screen, even if\n",
    "#   you have redirected stdout to a file.  And they will not be seen by\n",
    "#   the autograder, so the autograder won't be confused by them.\n",
    "# \n",
    "# - You can configure how much logging information is provided, by\n",
    "#   controlling the logging 'level'. You have a few options, like\n",
    "#   'debug', 'info', 'warning', and 'error'. By setting a global flag,\n",
    "#   you can ensure that the information you want - and only that info -\n",
    "#   is printed. As an example:\n",
    "#        >>> try:\n",
    "#        ...     rare_word = \"prestidigitation\"\n",
    "#        ...     vocab.get_counts(rare_word)\n",
    "#        ... except KeyError:\n",
    "#        ...     log.error(f\"Word that broke the program: {rare_word}\")\n",
    "#        ...     log.error(f\"Current contents of vocab: {vocab.data}\")\n",
    "#        ...     raise  # Crash the program; can't recover.\n",
    "#        >>> log.info(f\"Size of vocabulary is {len(vocab)}\")\n",
    "#        >>> if len(vocab) == 0:\n",
    "#        ...     log.warning(f\"Empty vocab. This may cause problems.\")\n",
    "#        >>> log.debug(f\"The values are {vocab}\")\n",
    "#   If we set the log level to be 'INFO', only the log.info, log.warning,\n",
    "#   and log.error statements will be printed. You can calibrate exactly how \n",
    "#   much info you need, and when. None of these pollute stdout with things \n",
    "#   that aren't the real 'output' of your program.\n",
    "# \n",
    "# In `parse_args`, we provided two command-line options to control the logging level.\n",
    "# The default level is 'INFO'. You can lower it to 'DEBUG' if you pass '--verbose'\n",
    "# and you can raise it to 'WARNING' if you pass '--quiet'.\n",
    "#\n",
    "# More info: https://docs.python.org/3/howto/logging.html#logging-basic-tutorial\n",
    "# \n",
    "# In all the starter code for the NLP course, we've elected to create a separate\n",
    "# logger for each source code file, stored in a variable named log that\n",
    "# is globally visible throughout the file.  That way, calls like log.info(...)\n",
    "# will use the logger for the current source code file and thus their output will \n",
    "# helpfully show the filename.  You could configure the current file's logger using\n",
    "# log.basicConfig(...), whereas logging.basicConfig(...) affects all of the loggers.\n",
    "# The command-line options affect all of the loggers.\n",
    "\n",
    "\n",
    "def parse_args() -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser(description=__doc__)\n",
    "    parser.add_argument(\"embeddings\", type=Path, help=\"path to word embeddings file\")\n",
    "    parser.add_argument(\"word\", type=str, help=\"word to look up\")\n",
    "    parser.add_argument(\"--minus\", type=str, default=None)\n",
    "    parser.add_argument(\"--plus\", type=str, default=None)\n",
    "\n",
    "    # for verbosity of logging\n",
    "    parser.set_defaults(logging_level=logging.INFO)\n",
    "    verbosity = parser.add_mutually_exclusive_group()\n",
    "    verbosity.add_argument(\n",
    "        \"-v\", \"--verbose\", dest=\"logging_level\", action=\"store_const\", const=logging.DEBUG\n",
    "    )\n",
    "    verbosity.add_argument(\n",
    "        \"-q\", \"--quiet\",   dest=\"logging_level\", action=\"store_const\", const=logging.WARNING\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    if not args.embeddings.is_file():\n",
    "        parser.error(f\"Embeddings file {args.embeddings} not found\")\n",
    "    if (args.minus is None) != (args.plus is None):  # != is the XOR operation!\n",
    "        parser.error(\"Must include both `--plus` and `--minus` or neither\")\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e7f445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "class Lexicon:\n",
    "    \"\"\"\n",
    "    Class that manages a lexicon and can compute similarity.\n",
    "\n",
    "    >>> my_lexicon = Lexicon.from_file(my_file)\n",
    "    >>> my_lexicon.find_similar_words(\"bagpipe\")\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, token_to_ix: Dict[str, int], ix_to_token: Dict[int, str],\n",
    "                 token_embeddings_tensor: torch.Tensor, embedding_norms: torch.Tensor) -> None:\n",
    "        \"\"\"Load information into coupled word-index mapping and embedding matrix.\"\"\"\n",
    "        # FINISH THIS FUNCTION\n",
    "        # Store your stuff! Both the word-index mapping and the embedding matrix.\n",
    "        #\n",
    "        # Do something with this size info?\n",
    "        # PyTorch's torch.Tensor objects rely on fixed-size arrays in memory.\n",
    "        # One of the worst things you can do for efficiency is\n",
    "        # append row-by-row, like you would with a Python list.\n",
    "        #\n",
    "        # Probably make the entire list all at once, then convert to a torch.Tensor.\n",
    "        # Otherwise, make the torch.Tensor and overwrite its contents row-by-row.\n",
    "        self.token_to_ix = token_to_ix\n",
    "        self.ix_to_token = ix_to_token\n",
    "        self.token_embeddings_tensor = token_embeddings_tensor\n",
    "        self.embedding_norms = embedding_norms\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, file: Path) -> Lexicon:\n",
    "        return Lexicon.WordEmbeddingBuilder(file).build()\n",
    "\n",
    "    class WordEmbeddingBuilder:\n",
    "        def __init__(self, file_path: Path, normalize: bool = True):\n",
    "            self.file_path = file_path\n",
    "            self._normalized = normalize\n",
    "            \n",
    "        def _normalize(self, embedding: List[float]) -> List[float]:\n",
    "            return embedding / np.linalg.norm(embedding)\n",
    "        \n",
    "        def build(self):\n",
    "            token_list = []\n",
    "            embedding_list = []\n",
    "\n",
    "            with open(self.file_path) as f:\n",
    "                first_line = next(f)  # Peel off the special first line.\n",
    "                for line in f:  # All of the other lines are regular.\n",
    "                    splits = line.strip().split()\n",
    "                    if len(splits) == 0:\n",
    "                        continue\n",
    "                    token = splits[0] \n",
    "                    embedding = [float(split) for split in splits[1:]]\n",
    "                    \n",
    "                    token_list.append(token)\n",
    "                    embedding_list.append(embedding)\n",
    "                \n",
    "                token_to_ix = dict(enumerate(token_list))\n",
    "                index_to_token = {token: index for index, token in enumerate(token_list)}\n",
    "                \n",
    "                token_embeddings_tensor = torch.tensor(embedding_list, dtype=torch.float32)\n",
    "\n",
    "                if self._normalized:\n",
    "                    # for numerical stability\n",
    "                    normalized = token_embeddings_tensor.norm(dim=1, keepdim=True)\n",
    "                    token_embeddings_tensor = token_embeddings_tensor / (normalized + 1e-16)\n",
    "\n",
    "                return Lexicon(token_to_ix, index_to_token, token_embeddings_tensor, token_embeddings_tensor.norm(dim=1))  \n",
    "        \n",
    "    def find_similar_words(\n",
    "        self, word: str, *, plus: Optional[str] = None, minus: Optional[str] = None\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Find most similar words, in terms of embeddings, to a query.\"\"\"\n",
    "        # FINISH THIS FUNCTION\n",
    "\n",
    "        # The star above forces you to use `plus` and `minus` only\n",
    "        # as named arguments. This helps avoid mixups or readability\n",
    "        # problems where you forget which comes first.\n",
    "        #\n",
    "        # We've also given `plus` and `minus` the type annotation\n",
    "        # Optional[str]. This means that the argument may be None, or\n",
    "        # it may be a string. If you don't provide these, it'll automatically\n",
    "        # use the default value we provided: None.\n",
    "        if (minus is None) != (plus is None):  # != is the XOR operation!\n",
    "            raise TypeError(\"Must include both of `plus` and `minus` or neither.\")\n",
    "        # Keep going!\n",
    "        # Be sure that you use fast, batched computations\n",
    "        # instead of looping over the rows. If you use a loop or a comprehension\n",
    "        # in this function, you've probably made a mistake.\n",
    "        if word not in self.token_to_ix:\n",
    "            raise ValueError(f\"Word {word} not found in lexicon\")\n",
    "        \n",
    "        # Handle word analogy: word + plus - minus\n",
    "        if plus is not None and minus is not None:\n",
    "            if plus not in self.token_to_ix:\n",
    "                raise ValueError(f\"Word {plus} not found in lexicon\")\n",
    "            if minus not in self.token_to_ix:\n",
    "                raise ValueError(f\"Word {minus} not found in lexicon\")\n",
    "            \n",
    "            # Compute analogy: word + plus - minus\n",
    "            query_embedding = (self.token_embeddings_tensor[self.token_to_ix[word]] + \n",
    "                             self.token_embeddings_tensor[self.token_to_ix[plus]] - \n",
    "                             self.token_embeddings_tensor[self.token_to_ix[minus]])\n",
    "            # Normalize the query embedding\n",
    "            query_embedding = query_embedding / (torch.norm(query_embedding) + 1e-16)\n",
    "        else:\n",
    "            # Regular similarity search\n",
    "            query_ix = self.token_to_ix[word]\n",
    "            query_embedding = self.token_embeddings_tensor[query_ix]\n",
    "        \n",
    "        # Compute cosine similarities using matrix multiplication\n",
    "        # Since embeddings are already normalized, this gives us cosine similarity\n",
    "        similarities = torch.matmul(self.token_embeddings_tensor, query_embedding)\n",
    "        \n",
    "        # Exclude the query word itself from results\n",
    "        if plus is None and minus is None:  # Only exclude for regular similarity, not analogy\n",
    "            query_ix = self.token_to_ix[word]\n",
    "            similarities[query_ix] = -float('inf')\n",
    "        \n",
    "        # Get top 10 most similar words\n",
    "        topk_ix = torch.topk(similarities, k=10).indices\n",
    "        \n",
    "        return [self.ix_to_token[ix.item()] for ix in topk_ix]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d69d8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple, banana, orange'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join(['apple', 'banana', 'orange'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41dc02a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--minus MINUS] [--plus PLUS] [-v | -q]\n",
      "                             embeddings word\n",
      "ipykernel_launcher.py: error: the following arguments are required: embeddings, word\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/nlp-class/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args = parse_args()\n",
    "    logging.basicConfig(level=args.logging_level)\n",
    "    lexicon = Lexicon.from_file(args.embeddings)\n",
    "    similar_words = lexicon.find_similar_words(\n",
    "        args.word, plus=args.plus, minus=args.minus\n",
    "    )\n",
    "    print(\" \".join(similar_words))  # print all words on one line, separated by spaces\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d3f304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = [\"apple\", \"banana\", \"orange\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd3580",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_ix = dict(enumerate(token_list))\n",
    "index_to_token = {token: index for token, index in token_to_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6ad06ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'apple', 1: 'banana', 2: 'orange'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(enumerate(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a21ebd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'apple', 1: 'banana', 2: 'orange'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "088ada8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': 0, 'banana': 1, 'orange': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5fe8a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from findsim import Lexicon\n",
    "def findsim(file_path, word, plus=None, minus=None):\n",
    "    lexicon = Lexicon.from_file(file_path)\n",
    "    similar_words = lexicon.find_similar_words(\n",
    "        word, plus=plus, minus=minus\n",
    "    )\n",
    "    print(\" \".join(similar_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f46ade2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 10\n",
      "Word: seattle\n",
      "seattle indianapolis atlanta lakers dallas expos boston detroit cleveland houston\n",
      "Word: dog\n",
      "dog turnip coronets ass pig embroidered eyed cow unicorns haired\n",
      "Word: communist\n",
      "communist socialist rightist udt comintern communists fascist instigated bolshevik leftist\n",
      "Word: jpg\n",
      "jpg hout maui hino ledger lsch storey bahnhof longship monte\n",
      "Word: the\n",
      "the marked reintroduced successive split gradually contention sway intervening changed\n",
      "Word: google\n",
      "google info geocaching archiving downloadable digitized web com printing bitnet\n",
      "\n",
      "\n",
      "d = 20\n",
      "Word: seattle\n",
      "seattle atlanta dallas miami canucks indianapolis cincinnati lauderdale florida knicks\n",
      "Word: dog\n",
      "dog cat dogs ass badger hound sighthound canine azawakh bikini\n",
      "Word: communist\n",
      "communist socialist communists comintern fascist trotskyist cnt agitating bolshevik nationalist\n",
      "Word: jpg\n",
      "jpg png colspan ffffff bratislava verbena gallery hoek alster hopewell\n",
      "Word: the\n",
      "the of in within its between entire over part uninterrupted\n",
      "Word: google\n",
      "google geocaching portals info web yahoo directory dmoz com wiki\n",
      "\n",
      "\n",
      "d = 50\n",
      "Word: seattle\n",
      "seattle seahawks dallas atlanta wichita tacoma lauderdale florida spokane chino\n",
      "Word: dog\n",
      "dog dogs badger cat hound puppy dachshund sighthound poodle rat\n",
      "Word: communist\n",
      "communist socialist bolshevik communists comintern trotskyist leftist bolsheviks cominform stalinist\n",
      "Word: jpg\n",
      "jpg png svg szczepanek buteo pix gif image galleria regnum\n",
      "Word: the\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mfindsim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m, in \u001b[0;36mfindsim\u001b[0;34m(file_path, word, plus, minus)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfindsim\u001b[39m(file_path, word, plus\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, minus\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 3\u001b[0m     lexicon \u001b[38;5;241m=\u001b[39m \u001b[43mLexicon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     similar_words \u001b[38;5;241m=\u001b[39m lexicon\u001b[38;5;241m.\u001b[39mfind_similar_words(\n\u001b[1;32m      5\u001b[0m         word, plus\u001b[38;5;241m=\u001b[39mplus, minus\u001b[38;5;241m=\u001b[39mminus\n\u001b[1;32m      6\u001b[0m     )\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(similar_words))\n",
      "File \u001b[0;32m~/Documents/JHU/Courses/NLP/NLP-JHU/hw-prob/hw-prob/findsim.py:130\u001b[0m, in \u001b[0;36mLexicon.from_file\u001b[0;34m(cls, file)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_file\u001b[39m(\u001b[38;5;28mcls\u001b[39m, file: Path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Lexicon:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLexicon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWordEmbeddingBuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/JHU/Courses/NLP/NLP-JHU/hw-prob/hw-prob/findsim.py:155\u001b[0m, in \u001b[0;36mLexicon.WordEmbeddingBuilder.build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m     embedding_list\u001b[38;5;241m.\u001b[39mappend(embedding)\n\u001b[1;32m    153\u001b[0m vocab \u001b[38;5;241m=\u001b[39m Integerizer(token_list)\n\u001b[0;32m--> 155\u001b[0m token_embeddings_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalized:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# for numerical stability\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     normalized \u001b[38;5;241m=\u001b[39m token_embeddings_tensor\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "words = [\"seattle\", \"dog\", \"communist\", \"jpg\", \"the\", \"google\"]\n",
    "file_path = [\"lexicons/words-10.txt\", \"lexicons/words-20.txt\", \"lexicons/words-50.txt\", \n",
    "             \"lexicons/words-100.txt\", \"lexicons/words-200.txt\"]\n",
    "\n",
    "for file in file_path:\n",
    "    print(f\"d = {file.split('.')[0].split('-')[-1]}\")\n",
    "    for word in words:\n",
    "        print(f\"Word: {word}\")\n",
    "        findsim(file, word)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53b02bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d = 10\n",
      "bank - river + money\n",
      "funding gst bankruptcy credit reporting shareholder firm profit deferred seignorage\n",
      "apple - fruit + company\n",
      "ibm commodore pricewaterhousecoopers mca discontinued revamped isu honeywell tramiel amd\n",
      "python - animal + programming\n",
      "minix multiplan ms netbsd openstep odp ported fgu basica slate\n",
      "nurse - woman + man\n",
      "huck boyfriend phoney girlfriend housewife girlfriends bookish dugan belson obsessed\n",
      "beautiful - more + most\n",
      "supergroup goldsmiths beltaine dawn artworks minerva ecd coffins whirling tuileries\n",
      "\n",
      "\n",
      "d = 20\n",
      "bank - river + money\n",
      "credit creditors contracts taxpayer premiums compensation payment repayment profits repay\n",
      "apple - fruit + company\n",
      "tramiel microcomputer ibm buyout corp inprise commodore bushnell compaq mattel\n",
      "python - animal + programming\n",
      "terseness kernighan applescript irssi perl awk niklaus scripting rexx tads\n",
      "nurse - woman + man\n",
      "frink parris onetime lister bunter indefatigable doctor dentist spotswood groote\n",
      "beautiful - more + most\n",
      "quaint famous birthplace pied folktale pageants magnificent faraway finest revered\n",
      "\n",
      "\n",
      "d = 50\n",
      "bank - river + money\n",
      "repayment repay funds creditors borrowers payments receipts repayments payment lending\n",
      "apple - fruit + company\n",
      "corp mattel tramiel inprise compaq fairchild intellivision buyout worldcom bushnell\n",
      "python - animal + programming\n",
      "perl haskell lisp smalltalk japh ocaml scripting monty tcl fortran\n",
      "nurse - woman + man\n",
      "doctor changeling bedside dentist apprentice veterinarian technician bunter speechless vindictive\n",
      "beautiful - more + most\n",
      "famous birthplace magnificent renowned finest famed fountains mausoleums pied immortalized\n",
      "\n",
      "\n",
      "d = 100\n",
      "bank - river + money\n",
      "payments repay funds lender lending payment debts credit dividends financial\n",
      "apple - fruit + company\n",
      "verizon corp compaq lucent fairchild datapoint tramiel mattel commodore eff\n",
      "python - animal + programming\n",
      "scripting perl ecmascript smalltalk javascript tcl ocaml kernighan haskell applescript\n",
      "nurse - woman + man\n",
      "nurses doctor gardener changeling dentist apprentice kesey technician butcher crying\n",
      "beautiful - more + most\n",
      "famous finest waterfalls magnificent renowned beauties beauty beaches fountains masterpiece\n",
      "\n",
      "\n",
      "d = 200\n",
      "bank - river + money\n",
      "funds lending bankers payments fund payment loans creditors savings financial\n",
      "apple - fruit + company\n",
      "tramiel corp compaq lucent honeywell corporation verizon hasbro atari sgi\n",
      "python - animal + programming\n",
      "tcl smalltalk scripting brainfuck ecmascript javascript prolog rdbms haskell rexx\n",
      "nurse - woman + man\n",
      "overwork nurses friend doctor nursing wits crane wingfield beregond pathologist\n",
      "beautiful - more + most\n",
      "famous finest masterpieces beauty waterfalls magnificent handsome beauties beaches admirers\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = [(\"bank\", \"river\", \"money\"), (\"apple\", \"fruit\", \"company\"),\n",
    "         (\"python\", \"animal\", \"programming\"), (\"nurse\", \"woman\", \"man\"),\n",
    "         (\"beautiful\", \"more\", \"most\")]\n",
    "file_path = [\"lexicons/words-10.txt\", \"lexicons/words-20.txt\", \"lexicons/words-50.txt\", \n",
    "             \"lexicons/words-100.txt\", \"lexicons/words-200.txt\"]\n",
    "\n",
    "for file in file_path:\n",
    "    print(f\"d = {file.split('.')[0].split('-')[-1]}\")\n",
    "    for word in words:\n",
    "        print(f\"{word[0]} - {word[1]} + {word[2]}\")\n",
    "        findsim(file, word[0], minus=word[1], plus=word[2])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "846b9b5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bank' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbank\u001b[49m \u001b[38;5;241m-\u001b[39m river \u001b[38;5;241m+\u001b[39m money\n\u001b[1;32m      2\u001b[0m apple \u001b[38;5;241m-\u001b[39m fruit \u001b[38;5;241m+\u001b[39m company\n\u001b[1;32m      3\u001b[0m python \u001b[38;5;241m-\u001b[39m animal \u001b[38;5;241m+\u001b[39m programming\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bank' is not defined"
     ]
    }
   ],
   "source": [
    "bank - river + money\n",
    "apple - fruit + company\n",
    "python - animal + programming\n",
    "nurse - woman + man\n",
    "beautiful - more + most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410079e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
