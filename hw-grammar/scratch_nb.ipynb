{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b03d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from collections import ChainMap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b86db66",
   "metadata": {},
   "source": [
    "#### Create grammar with terminal, non-terminal hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0caf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_terminal_pattern = r\"^[A-Z]+$\"\n",
    "pre_terminal_pattern = r\"^[A-Za-z]{2,}$\"\n",
    "terminal_pattern = r\"(^[a-z\\s]+)$\"\n",
    "\n",
    "def _spit_probable_choice(items, weights):\n",
    "    return random.choices(items, weights, k=1)[0]\n",
    "\n",
    "grammar_text = open(\"grammar.gr\", \"rb\").read().decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def _is_non_terminal(text):\n",
    "    return bool(re.search(non_terminal_pattern, text))\n",
    "\n",
    "def _is_pre_terminal(text):\n",
    "    return bool(re.search(pre_terminal_pattern, text))\n",
    "\n",
    "def _is_terminal(text):\n",
    "    return bool(re.search(terminal_pattern, text))\n",
    "\n",
    "def _create_grammar_hash(grammar_text):\n",
    "    def _is_comment(line):\n",
    "        if re.search(r\"^[#\\s()]\", line):\n",
    "            return 1\n",
    "\n",
    "    valid_symbols = list(filter(lambda text: len(text)>1 and not _is_comment(text), grammar_text.split(\"\\n\")))\n",
    "    cleaned_valid_symbols = list(map(lambda line: line.split(\"#\")[0].strip().split(\"\\t\"), valid_symbols))\n",
    "\n",
    "    non_terminal_hash = {}\n",
    "    terminal_hash = {}\n",
    "    for symbol in cleaned_valid_symbols:\n",
    "        if _is_terminal(symbol[2]):\n",
    "            if symbol[1] not in terminal_hash:\n",
    "                terminal_hash[symbol[1]] = {str(symbol[2]): float(symbol[0])}\n",
    "                continue\n",
    "            terminal_hash[symbol[1]].update({str(symbol[2]): float(symbol[0])})\n",
    "            continue\n",
    "        else:\n",
    "            print(symbol)\n",
    "            if symbol[1] not in non_terminal_hash:\n",
    "                non_terminal_hash[symbol[1]] = {tuple(map(str, symbol[2].split())): float(symbol[0])}\n",
    "                continue\n",
    "            non_terminal_hash[symbol[1]].update({tuple(map(str, symbol[2].split())): float(symbol[0])})\n",
    "    merged = non_terminal_hash.copy()\n",
    "    merged.update(terminal_hash)\n",
    "    return terminal_hash, non_terminal_hash, merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de11315",
   "metadata": {},
   "source": [
    "#### Create a combined grammar hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_terminal_pattern = r\"^[A-Z]+$\"\n",
    "pre_terminal_pattern = r\"^[A-Za-z]{2,}$\"\n",
    "terminal_pattern = r\"(^[a-z\\s]+)$\"\n",
    "\n",
    "def _spit_probable_choice(items, weights):\n",
    "    return random.choices(items, weights, k=1)[0]\n",
    "\n",
    "grammar_text = open(\"grammar.gr\", \"rb\").read().decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def _is_non_terminal(text):\n",
    "    return bool(re.search(non_terminal_pattern, text))\n",
    "\n",
    "def _is_pre_terminal(text):\n",
    "    return bool(re.search(pre_terminal_pattern, text))\n",
    "\n",
    "def _is_terminal(text):\n",
    "    return bool(re.search(terminal_pattern, text))\n",
    "\n",
    "def _create_grammar_hash(grammar_text):\n",
    "    def _is_comment(line):\n",
    "        if re.search(r\"^[#\\s()]\", line):\n",
    "            return 1\n",
    "\n",
    "    valid_symbols = list(filter(lambda text: len(text)>1 and not _is_comment(text), grammar_text.split(\"\\n\")))\n",
    "    cleaned_valid_symbols = list(map(lambda line: line.split(\"#\")[0].strip().split(\"\\t\"), valid_symbols))\n",
    "\n",
    "    grammar_hash = {}\n",
    "    for symbol in cleaned_valid_symbols:\n",
    "        if _is_terminal(symbol[2]):        \n",
    "            if symbol[1] not in grammar_hash:\n",
    "                grammar_hash[symbol[1]] = {str(symbol[2]): float(symbol[0])}\n",
    "                continue\n",
    "            grammar_hash[symbol[1]].update({str(symbol[2]): float(symbol[0])})\n",
    "            continue\n",
    "        else:\n",
    "            if symbol[1] not in grammar_hash:\n",
    "                grammar_hash[symbol[1]] = {tuple(map(str, symbol[2].split())): float(symbol[0])}\n",
    "                continue\n",
    "            grammar_hash[symbol[1]].update({tuple(map(str, symbol[2].split())): float(symbol[0])})\n",
    "            continue\n",
    "    return grammar_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1429ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_hash = _create_grammar_hash(grammar_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ddef07",
   "metadata": {},
   "source": [
    "#### Grammar Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c36cb6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grammar:\n",
    "    def __init__(self, grammar_file):\n",
    "        \"\"\"\n",
    "        Context-Free Grammar (CFG) Sentence Generator\n",
    "\n",
    "        Args:\n",
    "            grammar_file (str): Path to a .gr grammar file\n",
    "\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        # Parse the input grammar file\n",
    "        self.rules = None\n",
    "        self._load_rules_from_file(grammar_file)\n",
    "\n",
    "    @staticmethod\n",
    "    def _select_probable_choice(items, weights):\n",
    "        return random.choices(items, weights, k=1)[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_tokens_to_sentence(tokens):\n",
    "        sentence = \" \".join(tokens)\n",
    "        sentence = re.sub(r\"(?<!\\.)\\s+([.!?])\", r\"\\1\", sentence)\n",
    "        sentence = re.sub(r\"\\s+'\", \"'\", sentence)\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def _is_terminal(text):\n",
    "        return bool(re.search(terminal_pattern, text))\n",
    "\n",
    "    def _load_rules_from_file(self, grammar_file):\n",
    "        \"\"\"\n",
    "        Read grammar file and store its rules in self.rules\n",
    "\n",
    "        Args:\n",
    "            grammar_file (str): Path to the raw grammar file\n",
    "        \"\"\"\n",
    "\n",
    "        def _is_comment(line):\n",
    "            if re.search(r\"^[#\\s()]\", line):\n",
    "                return 1\n",
    "\n",
    "        grammar_text = open(grammar_file, \"rb\").read().decode(\"utf-8\")\n",
    "        \n",
    "        valid_symbols = list(\n",
    "            filter(\n",
    "                lambda text: len(text) > 1 and not _is_comment(text),\n",
    "                grammar_text.split(\"\\n\"),\n",
    "            )\n",
    "        )\n",
    "        cleaned_valid_symbols = list(\n",
    "            map(lambda line: line.split(\"#\")[0].strip().split(\"\\t\"), valid_symbols)\n",
    "        )\n",
    "\n",
    "        grammar_hash = {}\n",
    "        for symbol in cleaned_valid_symbols:\n",
    "            if self._is_terminal(symbol[2]):\n",
    "                if symbol[1] not in grammar_hash:\n",
    "                    grammar_hash[symbol[1]] = {str(symbol[2]): float(symbol[0])}\n",
    "                    continue\n",
    "                grammar_hash[symbol[1]].update({str(symbol[2]): float(symbol[0])})\n",
    "                continue\n",
    "            else:\n",
    "                if symbol[1] not in grammar_hash:\n",
    "                    grammar_hash[symbol[1]] = {\n",
    "                        tuple(map(str, symbol[2].split())): float(symbol[0])\n",
    "                    }\n",
    "                    continue\n",
    "                grammar_hash[symbol[1]].update(\n",
    "                    {tuple(map(str, symbol[2].split())): float(symbol[0])}\n",
    "                )\n",
    "                continue\n",
    "        self.rules = grammar_hash\n",
    "\n",
    "    def sample(self, derivation_tree, max_expansions, start_symbol):\n",
    "        \"\"\"\n",
    "        Sample a random sentence from this grammar\n",
    "\n",
    "        Args:\n",
    "            derivation_tree (bool): if true, the returned string will represent\n",
    "                the tree (using bracket notation) that records how the sentence\n",
    "                was derived\n",
    "            max_expansions (int): max number of nonterminal expansions we allow\n",
    "\n",
    "            start_symbol (str): start symbol to generate from\n",
    "\n",
    "        Returns:\n",
    "            str: the random sentence or its derivation tree\n",
    "        \"\"\"\n",
    "        if start_symbol not in self.rules:\n",
    "            return start_symbol.strip()\n",
    "\n",
    "        def _is_nonterminal(sym):\n",
    "            return sym.isupper()\n",
    "        \n",
    "        n_expansions = 0\n",
    "        def _tree_expand(symbol):\n",
    "            nonlocal n_expansions\n",
    "\n",
    "            if symbol not in self.rules:\n",
    "                tokens = symbol.split()\n",
    "                return tokens, symbol\n",
    "\n",
    "            if _is_nonterminal(symbol):\n",
    "                n_expansions += 1\n",
    "                if n_expansions > max_expansions:\n",
    "                    return [\"...\"], f\"({symbol} ...)\"\n",
    "\n",
    "            items = list(self.rules[symbol].keys())\n",
    "            weights = list(self.rules[symbol].values())\n",
    "            right_split = self._select_probable_choice(items=items, weights=weights)\n",
    "\n",
    "            if isinstance(right_split, (tuple, list)):\n",
    "                tokens_list, subtrees = [], []                \n",
    "                for daughter in right_split:\n",
    "                    if (daughter in self.rules) and \\\n",
    "                        (_is_nonterminal(daughter)) and \\\n",
    "                            (n_expansions >= max_expansions):\n",
    "                        tokens_list.append(\"...\")\n",
    "                        subtrees.append(\"...\")\n",
    "                        continue\n",
    "                    tokens, subtree = _tree_expand(daughter)\n",
    "                    tokens_list.extend(tokens)\n",
    "                    subtrees.append(subtree)\n",
    "                return tokens_list, f\"({symbol} {' '.join(subtrees)})\"\n",
    "            else:\n",
    "                return right_split.split(), f\"({symbol} {right_split})\"\n",
    "        \n",
    "        tokens, tree_str = _tree_expand(start_symbol)\n",
    "        return tree_str if derivation_tree else self._convert_tokens_to_sentence(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c55c25f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(ROOT (S (NP ... ...) ...) .)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar = Grammar(grammar_file=\"grammar.gr\")\n",
    "grammar.sample(derivation_tree=True, max_expansions=3, start_symbol=\"ROOT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e62e7ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. every pickle in every delicious pickle in the pickled chief of staff ate every sandwich on every sandwich under every floor on a pickle in a floor under the chief of staff on the floor on a president on a pickle!\n",
      "2. is it true that a pickle on the pickle with every pickle kissed every floor?\n",
      "3. every chief of staff with a pickle in a pickle in a floor in every president under the sandwich on a floor under every president in the pickle under the delicious sandwich in the sandwich under the president on the chief of staff in every president on every fine perplexed pickle under a pickle with the chief of staff understood a floor on the delicious sandwich with the pickle with the pickled floor!\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"{i+1}. {grammar.sample(derivation_tree=False, max_expansions=450, start_symbol='ROOT')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
