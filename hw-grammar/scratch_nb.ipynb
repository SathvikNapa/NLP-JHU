{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8caf0f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "146cd386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 4), match='TELL'>\n",
      "<re.Match object; span=(0, 4), match='Tell'>\n",
      "<re.Match object; span=(0, 14), match='chief of staff'>\n",
      "<re.Match object; span=(0, 4), match='tell'>\n"
     ]
    }
   ],
   "source": [
    "print(re.search(non_terminal_pattern, \"TELL\"))\n",
    "print(re.search(pre_terminal_pattern, \"Tell\"))\n",
    "print(re.search(terminal_pattern, \"chief of staff\"))\n",
    "print(re.search(terminal_pattern, \"tell\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ea46f87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ROOT': {('S', '.'): 1.0,\n",
       "  ('S', '!'): 1.0,\n",
       "  ('is', 'it', 'true', 'that', 'S', '?'): 1.0},\n",
       " 'S': {('NP', 'VP'): 1.0},\n",
       " 'VP': {('Verb', 'NP'): 1.0},\n",
       " 'NP': {('Det', 'Noun'): 1.0, ('NP', 'PP'): 1.0},\n",
       " 'PP': {('Prep', 'NP'): 1.0},\n",
       " 'Noun': {'president': 1.0,\n",
       "  'sandwich': 1.0,\n",
       "  'pickle': 1.0,\n",
       "  'chief of staff': 1.0,\n",
       "  'floor': 1.0},\n",
       " 'Verb': {'ate': 1.0,\n",
       "  'wanted': 1.0,\n",
       "  'kissed': 1.0,\n",
       "  'understood': 1.0,\n",
       "  'pickled': 1.0},\n",
       " 'Det': {'the': 1.0, 'a': 1.0, 'every': 1.0},\n",
       " 'Adj': {'fine': 1.0, 'delicious': 1.0, 'perplexed': 1.0, 'pickled': 1.0},\n",
       " 'Prep': {'with': 1.0, 'on': 1.0, 'under': 1.0, 'in': 1.0}}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivation_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0bd29c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8a0caf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from collections import ChainMap\n",
    "\n",
    "non_terminal_pattern = r\"^[A-Z]+$\"\n",
    "pre_terminal_pattern = r\"^[A-Za-z]{2,}$\"\n",
    "terminal_pattern = r\"(^[a-z\\s]+)$\"\n",
    "\n",
    "def _spit_probable_choice(items, weights):\n",
    "    return random.choices(items, weights, k=1)[0]\n",
    "\n",
    "grammar_text = open(\"grammar.gr\", \"rb\").read().decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def _is_non_terminal(text):\n",
    "    return bool(re.search(non_terminal_pattern, text))\n",
    "\n",
    "def _is_pre_terminal(text):\n",
    "    return bool(re.search(pre_terminal_pattern, text))\n",
    "\n",
    "def _is_terminal(text):\n",
    "    return bool(re.search(terminal_pattern, text))\n",
    "\n",
    "def _create_grammar_hash(grammar_text):\n",
    "    def _is_comment(line):\n",
    "        if re.search(r\"^[#\\s()]\", line):\n",
    "            return 1\n",
    "\n",
    "    valid_symbols = list(filter(lambda text: len(text)>1 and not _is_comment(text), grammar_text.split(\"\\n\")))\n",
    "    cleaned_valid_symbols = list(map(lambda line: line.split(\"#\")[0].strip().split(\"\\t\"), valid_symbols))\n",
    "\n",
    "    non_terminal_hash = {}\n",
    "    terminal_hash = {}\n",
    "    for symbol in cleaned_valid_symbols:\n",
    "        if _is_terminal(symbol[2]):\n",
    "            if symbol[1] not in terminal_hash:\n",
    "                terminal_hash[symbol[1]] = {str(symbol[2]): float(symbol[0])}\n",
    "                continue\n",
    "            terminal_hash[symbol[1]].update({str(symbol[2]): float(symbol[0])})\n",
    "            continue\n",
    "        else:\n",
    "            print(symbol)\n",
    "            if symbol[1] not in non_terminal_hash:\n",
    "                non_terminal_hash[symbol[1]] = {tuple(map(str, symbol[2].split())): float(symbol[0])}\n",
    "                continue\n",
    "            non_terminal_hash[symbol[1]].update({tuple(map(str, symbol[2].split())): float(symbol[0])})\n",
    "    merged = non_terminal_hash.copy()\n",
    "    merged.update(terminal_hash)\n",
    "    return terminal_hash, non_terminal_hash, merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c65a8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from collections import ChainMap\n",
    "\n",
    "non_terminal_pattern = r\"^[A-Z]+$\"\n",
    "pre_terminal_pattern = r\"^[A-Za-z]{2,}$\"\n",
    "terminal_pattern = r\"(^[a-z\\s]+)$\"\n",
    "\n",
    "def _spit_probable_choice(items, weights):\n",
    "    return random.choices(items, weights, k=1)[0]\n",
    "\n",
    "grammar_text = open(\"grammar.gr\", \"rb\").read().decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def _is_non_terminal(text):\n",
    "    return bool(re.search(non_terminal_pattern, text))\n",
    "\n",
    "def _is_pre_terminal(text):\n",
    "    return bool(re.search(pre_terminal_pattern, text))\n",
    "\n",
    "def _is_terminal(text):\n",
    "    return bool(re.search(terminal_pattern, text))\n",
    "\n",
    "def _create_grammar_hash(grammar_text):\n",
    "    def _is_comment(line):\n",
    "        if re.search(r\"^[#\\s()]\", line):\n",
    "            return 1\n",
    "\n",
    "    valid_symbols = list(filter(lambda text: len(text)>1 and not _is_comment(text), grammar_text.split(\"\\n\")))\n",
    "    cleaned_valid_symbols = list(map(lambda line: line.split(\"#\")[0].strip().split(\"\\t\"), valid_symbols))\n",
    "\n",
    "    grammar_hash = {}\n",
    "    for symbol in cleaned_valid_symbols:\n",
    "        if _is_terminal(symbol[2]):        \n",
    "            if symbol[1] not in grammar_hash:\n",
    "                grammar_hash[symbol[1]] = {str(symbol[2]): float(symbol[0])}\n",
    "                continue\n",
    "            grammar_hash[symbol[1]].update({str(symbol[2]): float(symbol[0])})\n",
    "            continue\n",
    "        else:\n",
    "            if symbol[1] not in grammar_hash:\n",
    "                grammar_hash[symbol[1]] = {tuple(map(str, symbol[2].split())): float(symbol[0])}\n",
    "                continue\n",
    "            grammar_hash[symbol[1]].update({tuple(map(str, symbol[2].split())): float(symbol[0])})\n",
    "            continue\n",
    "    return grammar_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1429ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_hash = _create_grammar_hash(grammar_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8735e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _build_sentence(non_terminal_hash, terminal_hash, symbol=\"ROOT\"):\n",
    "#     items, weights = list(map(lambda text: tuple(map(str, text)), list(non_terminal_hash[symbol].keys()))), list(non_terminal_hash[symbol].values())\n",
    "#     symbol_choice = _spit_probable_choice(items, weights)\n",
    "#     for word in symbol_choice:\n",
    "#         if word in (\".\", \"!\"):\n",
    "#             return word\n",
    "#         if not _is_terminal(word):\n",
    "#             if re.search(pre_terminal_pattern, word):\n",
    "#                 word = _spit_probable_choice(list(terminal_hash[word].keys()), \n",
    "#                                              list(terminal_hash[word].values()))\n",
    "#             else:\n",
    "#                 word = _spit_probable_choice(list(non_terminal_hash[word].keys()), \n",
    "#                                              list(non_terminal_hash[word].values()))\n",
    "#             return _build_sentence(non_terminal_hash, terminal_hash, word)\n",
    "#         else:\n",
    "#             return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0949d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'every chief of staff wanted every floor in every sandwich in the pickle on every pickle in every president under every president in the floor on a pickle under a pickle in a pickle on a chief of staff with every chief of staff under a president in a pickle on the chief of staff on every president with a floor with a chief of staff with the sandwich on a sandwich in every floor in the chief of staff in every pickle on every sandwich under every chief of staff with the chief of staff on every pickle on every sandwich in a pickle with the chief of staff under the floor under the chief of staff with the president under every chief of staff with every floor on the chief of staff with a president in a sandwich with a pickle on every floor with the sandwich in every sandwich with every chief of staff with a floor in the pickle in every president in the floor with a chief of staff with every floor in the pickle with the chief of staff on the chief of staff on every president with a sandwich on a pickle on the president with a sandwich with every sandwich with a pickle with the sandwich on a floor on the pickle with every president on every sandwich under the floor in the chief of staff in every pickle with the pickle under a pickle with every floor under every sandwich on a president in the sandwich on the floor under a chief of staff in a sandwich.'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v2\n",
    "# def _build_sentence(non_terminal_hash, terminal_hash, symbol=\"ROOT\"):\n",
    "#     if symbol in terminal_hash:\n",
    "#         items = list(terminal_hash[symbol].keys())\n",
    "#         weights = list(terminal_hash[symbol].values())\n",
    "#         return _spit_probable_choice(items, weights)\n",
    "\n",
    "#     if symbol in non_terminal_hash:\n",
    "#         items = list(non_terminal_hash[symbol].keys())\n",
    "#         weights = list(non_terminal_hash[symbol].values())\n",
    "#         rhs = _spit_probable_choice(items, weights)\n",
    "\n",
    "#         parts = []\n",
    "#         for sym in rhs:\n",
    "#             if sym in {\".\", \"!\", \"?\"}:\n",
    "#                 parts.append(sym)\n",
    "#             else:\n",
    "#                 parts.append(_build_sentence(non_terminal_hash, terminal_hash, sym))\n",
    "\n",
    "#         sentence = \" \".join(parts)\n",
    "#         sentence = re.sub(r\"\\s+([.!?])\", r\"\\1\", sentence)\n",
    "#         return sentence\n",
    "\n",
    "#     return symbol\n",
    "\n",
    "# _build_sentence(non_terminal_hash, terminal_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a40786db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ROOT'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _convert_tokens_to_sentence(tokens):\n",
    "    sentence = \" \".join(tokens)\n",
    "    sentence = re.sub(r\"\\s+([.!?])\", r\"\\1\", sentence)\n",
    "    sentence = re.sub(r\"\\s+'\", \"'\", sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def _build_sentence(derivation_tree, symbol=\"ROOT\", _tokens=False):\n",
    "    if symbol not in derivation_tree:\n",
    "        return symbol.split() if _tokens else symbol\n",
    "\n",
    "    if _is_terminal(symbol):\n",
    "        items = list(derivation_tree[symbol].keys())\n",
    "        weights = list(derivation_tree[symbol].values())\n",
    "        return _spit_probable_choice(items, weights)\n",
    "\n",
    "    if _is_non_terminal(symbol) or _is_pre_terminal(symbol):\n",
    "        items = list(derivation_tree[symbol].keys())\n",
    "        weights = list(derivation_tree[symbol].values())\n",
    "        rhs = _spit_probable_choice(items, weights)\n",
    "\n",
    "        if isinstance(rhs, tuple):\n",
    "            tokens = []\n",
    "            for sym in rhs:\n",
    "                # tokens.extend(_build_sentence(derivation_tree, sym))\n",
    "                part = _build_sentence(derivation_tree, sym, _tokens=True)\n",
    "                if isinstance(part, list):\n",
    "                    tokens.extend(part)\n",
    "                else:\n",
    "                    tokens.extend(part.split())\n",
    "\n",
    "            if _tokens:\n",
    "                return tokens\n",
    "            \n",
    "            return _convert_tokens_to_sentence(tokens)\n",
    "        else:\n",
    "            if _tokens:\n",
    "                return rhs.split()\n",
    "            s = rhs\n",
    "            # s = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", s)\n",
    "            # s = re.sub(r\"\\s+'\", \"'\", s)\n",
    "            # return s.strip()\n",
    "            return _convert_tokens_to_sentence([s])\n",
    "\n",
    "_build_sentence(derivation_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1cafddd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is it true that every sandwich under a floor with every president kissed every perplexed pickle?'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## V3\n",
    "def _build_sentence(derivation_tree, symbol=\"ROOT\", _tokens=False):\n",
    "    if symbol not in derivation_tree:\n",
    "        return symbol.split() if _tokens else symbol\n",
    "\n",
    "    ## handling non terminal\n",
    "    ### being safe\n",
    "    # choices = derivation_tree[symbol]\n",
    "    # if not choices:\n",
    "    #     return [] if _tokens else \"\"\n",
    "\n",
    "    ### Logic\n",
    "    items = list(derivation_tree[symbol].keys())\n",
    "    weights = list(derivation_tree[symbol].values())\n",
    "    rhs = _spit_probable_choice(items, weights)\n",
    "\n",
    "    if isinstance(rhs, tuple):\n",
    "        tokens = []\n",
    "        for sym in rhs:\n",
    "            tokens.extend(_build_sentence(derivation_tree, sym, _tokens=True))\n",
    "        return tokens if _tokens else _convert_tokens_to_sentence(tokens)\n",
    "    return rhs.split() if _tokens else rhs.strip()\n",
    "\n",
    "_build_sentence(grammar_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6cca1a",
   "metadata": {},
   "source": [
    "| Non Terminal | Symbol | probability |\n",
    "|--------------|--------|-------------|\n",
    "| ROOT         | S .    | 0.33        |\n",
    "| ROOT         | S !    | 0.33        |\n",
    "| ROOT         | is it true that S ? | 0.33 |\n",
    "| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36cb6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grammar:\n",
    "    def __init__(self, grammar_file):\n",
    "        \"\"\"\n",
    "        Context-Free Grammar (CFG) Sentence Generator\n",
    "\n",
    "        Args:\n",
    "            grammar_file (str): Path to a .gr grammar file\n",
    "        \n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        # Parse the input grammar file\n",
    "        self.rules = None\n",
    "        self._load_rules_from_file(grammar_file)\n",
    "\n",
    "    @staticmethod\n",
    "    def _spit_probable_choice(items, weights):\n",
    "        return random.choices(items, weights, k=1)[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_tokens_to_sentence(tokens):\n",
    "        sentence = \" \".join(tokens)\n",
    "        sentence = re.sub(r\"\\s+([.!?])\", r\"\\1\", sentence)\n",
    "        sentence = re.sub(r\"\\s+'\", \"'\", sentence)\n",
    "        return sentence\n",
    "\n",
    "    def _load_rules_from_file(self, grammar_file):\n",
    "        \"\"\"\n",
    "        Read grammar file and store its rules in self.rules\n",
    "\n",
    "        Args:\n",
    "            grammar_file (str): Path to the raw grammar file \n",
    "        \"\"\"\n",
    "        def _is_comment(line):\n",
    "            if re.search(r\"^[#\\s()]\", line):\n",
    "                return 1\n",
    "\n",
    "        grammar_text = open(grammar_file, \"rb\").read().decode(\"utf-8\")\n",
    "\n",
    "        def _is_terminal(text):\n",
    "            return bool(re.search(terminal_pattern, text))\n",
    "\n",
    "\n",
    "        valid_symbols = list(filter(lambda text: len(text)>1 and not _is_comment(text), grammar_text.split(\"\\n\")))\n",
    "        cleaned_valid_symbols = list(map(lambda line: line.split(\"#\")[0].strip().split(\"\\t\"), valid_symbols))\n",
    "\n",
    "        grammar_hash = {}\n",
    "        for symbol in cleaned_valid_symbols:\n",
    "            if _is_terminal(symbol[2]):        \n",
    "                if symbol[1] not in grammar_hash:\n",
    "                    grammar_hash[symbol[1]] = {str(symbol[2]): float(symbol[0])}\n",
    "                    continue\n",
    "                grammar_hash[symbol[1]].update({str(symbol[2]): float(symbol[0])})\n",
    "                continue\n",
    "            else:\n",
    "                if symbol[1] not in grammar_hash:\n",
    "                    grammar_hash[symbol[1]] = {tuple(map(str, symbol[2].split())): float(symbol[0])}\n",
    "                    continue\n",
    "                grammar_hash[symbol[1]].update({tuple(map(str, symbol[2].split())): float(symbol[0])})\n",
    "                continue\n",
    "        self.rules = grammar_hash\n",
    "\n",
    "    def sample(self, derivation_tree, max_expansions, start_symbol):\n",
    "        \"\"\"\n",
    "        Sample a random sentence from this grammar\n",
    "\n",
    "        Args:\n",
    "            derivation_tree (bool): if true, the returned string will represent \n",
    "                the tree (using bracket notation) that records how the sentence \n",
    "                was derived\n",
    "            max_expansions (int): max number of nonterminal expansions we allow\n",
    "\n",
    "            start_symbol (str): start symbol to generate from\n",
    "\n",
    "        Returns:\n",
    "            str: the random sentence or its derivation tree\n",
    "        \"\"\"\n",
    "        \n",
    "        if start_symbol not in self.rules:\n",
    "            return start_symbol.strip()\n",
    "        \n",
    "        def _tree_expand(symbol, n_expansions=0):\n",
    "            if (symbol not in self.rules) or (n_expansions >= max_expansions):\n",
    "                tokens = symbol.split()\n",
    "                tree = symbol\n",
    "                return tokens, tree\n",
    "            \n",
    "            items = list(self.rules[symbol].keys())\n",
    "            weights = list(self.rules[symbol].values())\n",
    "            right_split = self._spit_probable_choice(items=items, weights=weights)\n",
    "            \n",
    "            if isinstance(right_split, tuple):\n",
    "                tokens_list, subtrees = [], []\n",
    "                for child in right_split:\n",
    "                    tokens, tree = _tree_expand(child, n_expansions+1)\n",
    "                    tokens_list.extend(tokens)\n",
    "                    subtrees.append(tree)\n",
    "                return tokens_list, f\"({symbol} {' '.join(subtrees)})\"\n",
    "            else:\n",
    "                return right_split.split(), f\"({symbol} {right_split})\"\n",
    "        \n",
    "        tokens, tree_str = _tree_expand(start_symbol)\n",
    "        if derivation_tree:\n",
    "            return tree_str\n",
    "        return self._convert_tokens_to_sentence(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c55c25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = Grammar(grammar_file=\"grammar.gr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "79218a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is it true that the fine pickled president with a chief of staff on a chief of staff on NP PP Prep NP under the chief of staff under Det Noun in a chief of staff pickled a chief of staff on the floor with NP PP Prep NP on Det Noun Prep NP on Det Noun on a sandwich under the delicious sandwich?'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar.sample(derivation_tree=False, max_expansions=10, start_symbol=\"ROOT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53816d52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
