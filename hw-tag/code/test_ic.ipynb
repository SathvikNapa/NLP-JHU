{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T22:31:22.940022Z",
     "start_time": "2025-11-08T22:31:22.932751Z"
    }
   },
   "source": "##!/usr/bin/env python3",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file illustrates how you might experiment with the HMM interface.\n",
    "You can paste these commands in at the Python prompt, or execute `test_ic.py` directly.\n",
    "A notebook interface is nicer than the plain Python prompt, so we provide\n",
    "a notebook version of this file as `test_ic.ipynb`, which you can open with\n",
    "`jupyter` or with Visual Studio `code` (run it with the `nlp-class` kernel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T04:30:24.764737Z",
     "start_time": "2025-11-09T04:30:24.625483Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from torch import tensor"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tensor\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'numpy'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T22:33:21.205163Z",
     "start_time": "2025-11-08T22:33:21.125307Z"
    }
   },
   "source": [
    "from corpus import TaggedCorpus\n",
    "from eval import model_cross_entropy, write_tagging\n",
    "from hmm import HiddenMarkovModel\n",
    "from crf import ConditionalRandomField"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mcorpus\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m TaggedCorpus\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01meval\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m model_cross_entropy, write_tagging\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mhmm\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m HiddenMarkovModel\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mcrf\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ConditionalRandomField\n",
      "File \u001B[0;32m~/Documents/JHU/Courses/NLP/NLP-JHU/hw-tag/code/eval.py:15\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtqdm\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tqdm \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mcorpus\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Sentence, Word, EOS_WORD, BOS_WORD, OOV_WORD, TaggedCorpus\n\u001B[0;32m---> 15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mhmm\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m HiddenMarkovModel\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mintegerize\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Integerizer\n\u001B[1;32m     18\u001B[0m log \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(Path(\u001B[38;5;18m__file__\u001B[39m)\u001B[38;5;241m.\u001B[39mstem)  \u001B[38;5;66;03m# For usage, see findsim.py in earlier assignment.\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/JHU/Courses/NLP/NLP-JHU/hw-tag/code/hmm.py:35\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mcorpus\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     23\u001B[0m     BOS_TAG,\n\u001B[1;32m     24\u001B[0m     BOS_WORD,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     31\u001B[0m     Word,\n\u001B[1;32m     32\u001B[0m )\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mintegerize\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Integerizer\n\u001B[0;32m---> 35\u001B[0m TorchScalar \u001B[38;5;241m=\u001B[39m \u001B[43mFloat\u001B[49m\u001B[43m[\u001B[49m\u001B[43mTensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m  \u001B[38;5;66;03m# a Tensor with no dimensions, i.e., a scalar\u001B[39;00m\n\u001B[1;32m     37\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\n\u001B[1;32m     38\u001B[0m     Path(\u001B[38;5;18m__file__\u001B[39m)\u001B[38;5;241m.\u001B[39mstem\n\u001B[1;32m     39\u001B[0m )  \u001B[38;5;66;03m# For usage, see findsim.py in earlier assignment.\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Note: We use the name \"logger\" this time rather than \"log\" since we\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# are already using \"log\" for the mathematical log!\u001B[39;00m\n\u001B[1;32m     42\u001B[0m \n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# Set the seed for random numbers in torch, for replicability\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/JHU/Courses/NLP/NLP-JHU/hw-tag/venv/lib/python3.9/site-packages/jaxtyping/_array_types.py:669\u001B[0m, in \u001B[0;36m_MetaAbstractDtype.__getitem__\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m    667\u001B[0m         out \u001B[38;5;241m=\u001B[39m Union[out]\n\u001B[1;32m    668\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 669\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43m_make_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marray_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim_str\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtypes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    670\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m out \u001B[38;5;129;01mis\u001B[39;00m _not_made:\n\u001B[1;32m    671\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid jaxtyping type annotation.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/JHU/Courses/NLP/NLP-JHU/hw-tag/venv/lib/python3.9/site-packages/jaxtyping/_array_types.py:599\u001B[0m, in \u001B[0;36m_make_array\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    598\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_make_array\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 599\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43m_make_array_cached\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    601\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(out) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m:\n\u001B[1;32m    602\u001B[0m         array_type, name, dtypes, dims, index_variadic, dim_str \u001B[38;5;241m=\u001B[39m out\n",
      "File \u001B[0;32m~/Documents/JHU/Courses/NLP/NLP-JHU/hw-tag/venv/lib/python3.9/site-packages/jaxtyping/_array_types.py:552\u001B[0m, in \u001B[0;36m_make_array_cached\u001B[0;34m(array_type, dim_str, dtypes, name)\u001B[0m\n\u001B[1;32m    550\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    551\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _not_made\n\u001B[0;32m--> 552\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m array_type \u001B[38;5;129;01mis\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241m.\u001B[39mbool_:\n\u001B[1;32m    553\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _check_scalar(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbool\u001B[39m\u001B[38;5;124m\"\u001B[39m, dtypes, dims):\n\u001B[1;32m    554\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m array_type\n",
      "\u001B[0;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T22:32:12.980614Z",
     "start_time": "2025-11-08T22:32:12.975631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tagged_corpus = TaggedCorpus()\n",
    "sent = tagged_corpus.get_sentences()\n",
    "sent"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object TaggedCorpus.get_sentences at 0x12672bb30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(\"test_ic\")  # For usage, see findsim.py in earlier assignment.\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "logging.basicConfig(level=logging.INFO)  # could change INFO to DEBUG\n",
    "# torch.autograd.set_detect_anomaly(True)    # uncomment to improve error messages from .backward(), but slows down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch working directory to the directory where the data live.  You may want to edit this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get vocabulary and tagset from a supervised corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icsup = TaggedCorpus(Path(\"icsup\"), add_oov=False)\n",
    "log.info(f\"Ice cream vocabulary: {list(icsup.vocab)}\")\n",
    "log.info(f\"Ice cream tagset: {list(icsup.tagset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two ways to look at the corpus ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "os.system(\"cat icsup\")  # call the shell to look at the file directly"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "log.info(icsup)  # print the TaggedCorpus python object we constructed from it"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"*** Hidden Markov Model (HMM) test\\n\")\n",
    "hmm = HiddenMarkovModel(icsup.tagset, icsup.vocab)\n",
    "# Change the transition/emission initial probabilities to match the ice cream spreadsheet,\n",
    "# and test your implementation of the Viterbi algorithm.  Note that the spreadsheet \n",
    "# uses transposed versions of these matrices.\n",
    "hmm.B = tensor([[0.7000, 0.2000, 0.1000],  # emission probabilities\n",
    "                [0.1000, 0.2000, 0.7000],\n",
    "                [0.0000, 0.0000, 0.0000],\n",
    "                [0.0000, 0.0000, 0.0000]])\n",
    "hmm.A = tensor([[0.8000, 0.1000, 0.1000, 0.0000],  # transition probabilities\n",
    "                [0.1000, 0.8000, 0.1000, 0.0000],\n",
    "                [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "                [0.5000, 0.5000, 0.0000, 0.0000]])\n",
    "log.info(\"*** Current A, B matrices (using initalizations from the ice cream spreadsheet)\")\n",
    "hmm.printAB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out on the raw data from the spreadsheet, available in `icraw``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"*** Viterbi results on icraw with hard coded parameters\")\n",
    "icraw = TaggedCorpus(Path(\"icraw\"), tagset=icsup.tagset, vocab=icsup.vocab)\n",
    "write_tagging(hmm, icraw, Path(\"icraw_hmm.output\"))  # calls hmm.viterbi_tagging on each sentence\n",
    "os.system(\"cat icraw_hmm.output\")  # print the file we just created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the parameters that we guessed above get the \"correct\" answer, \n",
    "as revealed in `icdev`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icdev = TaggedCorpus(Path(\"icdev\"), tagset=icsup.tagset, vocab=icsup.vocab)\n",
    "log.info(f\"*** Compare to icdev corpus:\\n{icdev}\")\n",
    "from eval import viterbi_error_rate\n",
    "\n",
    "viterbi_error_rate(hmm, icdev, show_cross_entropy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try your training code, running it on supervised data.\n",
    "To test this, we'll restart from a random initialization.\n",
    "(You could also try creating this new model with `unigram=true`, \n",
    "which will affect the rest of the notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = HiddenMarkovModel(icsup.tagset, icsup.vocab)\n",
    "log.info(\"*** A, B matrices as randomly initialized close to uniform\")\n",
    "hmm.printAB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"*** Supervised HMM training on icsup\")\n",
    "cross_entropy_loss = lambda model: model_cross_entropy(model, icsup)\n",
    "hmm.train(corpus=icsup, loss=cross_entropy_loss, tolerance=0.0001)\n",
    "log.info(\"*** A, B matrices after training on icsup (should \"\n",
    "         \"match initial params on spreadsheet [transposed])\")\n",
    "hmm.printAB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've reached the spreadsheet's starting guess, let's again tag\n",
    "the spreadsheet \"sentence\" (that is, the sequence of ice creams) using the\n",
    "Viterbi algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"*** Viterbi results on icraw\")\n",
    "icraw = TaggedCorpus(Path(\"icraw\"), tagset=icsup.tagset, vocab=icsup.vocab)\n",
    "write_tagging(hmm, icraw, Path(\"icraw_hmm.output\"))  # calls hmm.viterbi_tagging on each sentence\n",
    "os.system(\"cat icraw_hmm.output\")  # print the file we just created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's use the forward algorithm to see what the model thinks about \n",
    "the probability of the spreadsheet \"sentence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"*** Forward algorithm on icraw (should approximately match iteration 0 \"\n",
    "         \"on spreadsheet)\")\n",
    "for sentence in icraw:\n",
    "    prob = math.exp(hmm.logprob(sentence, icraw))\n",
    "    log.info(f\"{prob} = p({sentence})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's reestimate on the icraw data, as the spreadsheet does.\n",
    "We'll evaluate as we go along on the *training* perplexity, and stop\n",
    "when that has more or less converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"*** Reestimating on icraw (perplexity should improve on every iteration)\")\n",
    "negative_log_likelihood = lambda model: model_cross_entropy(model, icraw)  # evaluate on icraw itself\n",
    "hmm.train(corpus=icraw, loss=negative_log_likelihood, tolerance=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"*** A, B matrices after reestimation on icraw \"\n",
    "         \"should match final params on spreadsheet [transposed])\")\n",
    "hmm.printAB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"*** Viterbi results on icraw after reestimation on icraw\")\n",
    "icraw = TaggedCorpus(Path(\"icraw\"), tagset=icsup.tagset, vocab=icsup.vocab)\n",
    "write_tagging(hmm, icraw, Path(\"icraw_hmm.output\"))  # calls hmm.viterbi_tagging on each sentence\n",
    "os.system(\"cat icraw_hmm.output\")  # print the file we just created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try out a randomly initialized CRF on the ice cream data. Notice how\n",
    "the initialized A and B matrices now hold non-negative potentials,\n",
    "rather than probabilities that sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"*** Conditional Random Field (CRF) test\\n\")\n",
    "crf = ConditionalRandomField(icsup.tagset, icsup.vocab)\n",
    "log.info(\"*** Current A, B matrices (potentials from small random parameters)\")\n",
    "crf.printAB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try your training code, running it on supervised data. To test this,\n",
    "we'll restart from a random initialization. \n",
    "\n",
    "Note that the logger reports the CRF's *conditional* cross-entropy, \n",
    "log p(tags | words) / n.  This is much lower than the HMM's *joint* \n",
    "cross-entropy log p(tags, words) / n, but that doesn't mean the CRF\n",
    "is worse at tagging.  The CRF is just predicting less information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"*** Supervised CRF training on icsup\")\n",
    "cross_entropy_loss = lambda model: model_cross_entropy(model, icsup)\n",
    "crf.train(corpus=icsup, loss=cross_entropy_loss, lr=0.1, tolerance=0.0001)\n",
    "log.info(\"*** A, B matrices after training on icsup\")\n",
    "crf.printAB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again tag the spreadsheet \"sentence\" (that is, the sequence of ice\n",
    "creams) using the Viterbi algorithm.  The trained CRF might get a different\n",
    "answer than the trained HMM.  (Try comparing the two icraw_*.output files.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"*** Viterbi results on icraw with trained parameters\")\n",
    "icraw = TaggedCorpus(Path(\"icraw\"), tagset=icsup.tagset, vocab=icsup.vocab)\n",
    "write_tagging(crf, icraw, Path(\"icraw_crf.output\"))  # calls hmm.viterbi_tagging on each sentence\n",
    "os.system(\"cat icraw_crf.output\")  # print the file we just created"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
