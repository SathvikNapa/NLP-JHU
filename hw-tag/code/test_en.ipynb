{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file illustrates how you might experiment with the HMM interface.\n",
    "You can paste these commands in at the Python prompt, or execute `test_en.py` directly.\n",
    "A notebook interface is nicer than the plain Python prompt, so we provide\n",
    "a notebook version of this file as `test_en.ipynb`, which you can open with\n",
    "`jupyter` or with Visual Studio `code` (run it with the `nlp-class` kernel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpus import TaggedCorpus\n",
    "from eval import eval_tagging, model_cross_entropy, viterbi_error_rate\n",
    "from hmm import HiddenMarkovModel\n",
    "from crf import ConditionalRandomField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.root.setLevel(level=logging.INFO)\n",
    "log = logging.getLogger(\"test_en\")       # For usage, see findsim.py in earlier assignment.\n",
    "logging.basicConfig(format=\"%(levelname)s : %(message)s\", level=logging.INFO)  # could change INFO to DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch working directory to the directory where the data live.  You may need to edit this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 191873 tokens from ensup, enraw\n",
      "INFO : Created 26 tag types\n",
      "INFO : Created 18461 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(entrain)=8064  len(ensup)=4051  len(endev)=996\n"
     ]
    }
   ],
   "source": [
    "entrain = TaggedCorpus(Path(\"ensup\"), Path(\"enraw\"))                               # all training\n",
    "ensup =   TaggedCorpus(Path(\"ensup\"), tagset=entrain.tagset, vocab=entrain.vocab)  # supervised training\n",
    "endev =   TaggedCorpus(Path(\"endev\"), tagset=entrain.tagset, vocab=entrain.vocab)  # evaluation\n",
    "print(f\"{len(entrain)=}  {len(ensup)=}  {len(endev)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 95936 tokens from ensup\n",
      "INFO : Created 26 tag types\n",
      "INFO : Created 12466 word types\n",
      "INFO : Tagset: f['W', 'J', 'N', 'C', 'V', 'I', 'D', ',', 'M', 'P', '.', 'E', 'R', '`', \"'\", 'T', '$', ':', '-', '#', 'S', 'F', 'U', 'L', '_EOS_TAG_', '_BOS_TAG_']\n"
     ]
    }
   ],
   "source": [
    "known_vocab = TaggedCorpus(Path(\"ensup\")).vocab    # words seen with supervised tags; used in evaluation\n",
    "log.info(f\"Tagset: f{list(entrain.tagset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an HMM.  Let's do some pre-training to approximately maximize the\n",
    "regularized log-likelihood on supervised training data.  In other words, the\n",
    "probabilities at the M step will just be supervised count ratios.\n",
    "\n",
    "On each epoch, you will see two progress bars: first it collects counts from\n",
    "all the sentences (E step), and then after the M step, it evaluates the loss\n",
    "function, which is the (unregularized) cross-entropy on the training set.\n",
    "\n",
    "The parameters don't actually matter during the E step because there are no\n",
    "hidden tags to impute.  The first M step will jump right to the optimal\n",
    "solution.  The code will try a second epoch with the revised parameters, but\n",
    "the result will be identical, so it will detect convergence and stop.\n",
    "\n",
    "We arbitrarily choose λ=1 for our add-λ smoothing at the M step, but it would\n",
    "be better to search for the best value of this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_log_likelihood = lambda model: model_cross_entropy(\n",
    "    model, icraw\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 73 tokens from icsup, icraw\n",
      "INFO : Created 4 tag types\n",
      "INFO : Created 6 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : *** Current A, B matrices (using initalizations from the ice cream spreadsheet)\n",
      "INFO : *** Viterbi results on icraw with hard coded parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(ictrain)=5  len(icsup)=4  len(icdev)=1\n",
      "Transition matrix A:\n",
      "\t(C|...)\t(H|...)\t(_EOS_TAG_|...)\t(_BOS_TAG_|...)\n",
      "C\t0.800\t0.100\t0.100\t0.000\n",
      "H\t0.100\t0.800\t0.100\t0.000\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.500\t0.500\t0.000\t0.000\n",
      "\n",
      "Emission matrix B:\n",
      "\t1\t2\t3\n",
      "C\t0.700\t0.200\t0.100\n",
      "H\t0.100\t0.200\t0.700\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.000\t0.000\t0.000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 30.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/H 3/H 3/H 2/H 3/H 2/H 3/H 2/H 2/H 3/H 1/H 3/H 3/H 1/C 1/C 1/C 2/C 1/C 1/C 1/C 3/C 1/C 2/C 1/C 1/C 1/C 2/H 3/H 3/H 2/H 3/H 2/H 2/H\n",
      "Transition matrix A:\n",
      "\t(C|...)\t(H|...)\t(_EOS_TAG_|...)\t(_BOS_TAG_|...)\n",
      "C\t0.800\t0.100\t0.100\t0.000\n",
      "H\t0.100\t0.800\t0.100\t0.000\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.500\t0.500\t0.000\t0.000\n",
      "\n",
      "Emission matrix B:\n",
      "\t1\t2\t3\n",
      "C\t0.700\t0.200\t0.100\n",
      "H\t0.100\t0.200\t0.700\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.000\t0.000\t0.000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 200.85it/s]\n",
      "INFO : Cross-entropy: 1.2217 nats (= perplexity 3.393)\n",
      "100%|██████████| 1/1 [00:00<00:00, 3609.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 619.54it/s]\n",
      "INFO : Cross-entropy: 1.7714 nats (= perplexity 5.879)\n",
      "INFO : Saved model to my_hmm.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix A:\n",
      "\t(C|...)\t(H|...)\t(_EOS_TAG_|...)\t(_BOS_TAG_|...)\n",
      "C\t0.333\t0.333\t0.333\t0.000\n",
      "H\t0.333\t0.333\t0.333\t0.000\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.333\t0.333\t0.333\t0.000\n",
      "\n",
      "Emission matrix B:\n",
      "\t1\t2\t3\t_OOV_\n",
      "C\t0.250\t0.250\t0.250\t0.250\n",
      "H\t0.250\t0.250\t0.250\t0.250\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.000\t0.000\t0.000\t0.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from eval import model_cross_entropy, write_tagging\n",
    "ictrain = TaggedCorpus(Path(\"icsup\"), Path(\"icraw\"))                               # all training\n",
    "icsup =   TaggedCorpus(Path(\"icsup\"), tagset=ictrain.tagset, vocab=ictrain.vocab)  # supervised training\n",
    "icdev =   TaggedCorpus(Path(\"icdev\"), tagset=ictrain.tagset, vocab=ictrain.vocab)  # evaluation\n",
    "\n",
    "print(f\"{len(ictrain)=}  {len(icsup)=}  {len(icdev)=}\")\n",
    "hmm = HiddenMarkovModel(icsup.tagset, icsup.vocab)\n",
    "hmm.B = torch.tensor(\n",
    "    [\n",
    "        [0.7000, 0.2000, 0.1000],  # emission probabilities\n",
    "        [0.1000, 0.2000, 0.7000],\n",
    "        [0.0000, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.0000],\n",
    "    ]\n",
    ")\n",
    "hmm.A = torch.tensor(\n",
    "    [\n",
    "        [0.8000, 0.1000, 0.1000, 0.0000],  # transition probabilities\n",
    "        [0.1000, 0.8000, 0.1000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
    "    ]\n",
    ")\n",
    "\n",
    "log.info(\n",
    "    \"*** Current A, B matrices (using initalizations from the ice cream spreadsheet)\"\n",
    ")\n",
    "hmm.printAB()\n",
    "\n",
    "# Try it out on the raw data from the spreadsheet, available in `icraw``.\n",
    "log.info(\"*** Viterbi results on icraw with hard coded parameters\")\n",
    "icraw = TaggedCorpus(Path(\"icraw\"), tagset=icsup.tagset, vocab=icsup.vocab)\n",
    "write_tagging(\n",
    "    hmm, icraw, Path(\"icraw_hmm.output\")\n",
    ")  # calls hmm.viterbi_tagging on each sentence\n",
    "os.system(\"cat icraw_hmm.output\")  # print the file we just created\n",
    "hmm.printAB()\n",
    "hmm.train(corpus=icraw, loss=negative_log_likelihood, tolerance=0.0001, max_steps=50000)\n",
    "hmm.printAB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 40 tokens from icsup\n",
      "INFO : Created 4 tag types\n",
      "INFO : Created 5 word types\n",
      "INFO : Ice cream vocabulary: ['1', '2', '3', '_EOS_WORD_', '_BOS_WORD_']\n",
      "INFO : Ice cream tagset: ['C', 'H', '_EOS_TAG_', '_BOS_TAG_']\n",
      "INFO : 1/C 1/C 1/C 1/C 1/C 1/C 1/C 2/C 2/C 3/H\n",
      "1/H 2/H 2/H 3/H 3/H 3/H 3/H 3/H 3/H 3/C\n",
      "1/C 1/C 1/C 1/C 1/C 1/C 1/C 2/C 2/C 3/H\n",
      "1/H 2/H 2/H 3/H 3/H 3/H 3/H 3/H 3/H 3/C\n",
      "INFO : *** Hidden Markov Model (HMM) test\n",
      "\n",
      "INFO : *** Current A, B matrices (using initalizations from the ice cream spreadsheet)\n",
      "INFO : *** Viterbi results on icraw with hard coded parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/C 1/C 1/C 1/C 1/C 1/C 1/C 2/C 2/C 3/H\n",
      "1/H 2/H 2/H 3/H 3/H 3/H 3/H 3/H 3/H 3/C\n",
      "1/C 1/C 1/C 1/C 1/C 1/C 1/C 2/C 2/C 3/H\n",
      "1/H 2/H 2/H 3/H 3/H 3/H 3/H 3/H 3/H 3/C\n",
      "Transition matrix A:\n",
      "\t(C|...)\t(H|...)\t(_EOS_TAG_|...)\t(_BOS_TAG_|...)\n",
      "C\t0.800\t0.100\t0.100\t0.000\n",
      "H\t0.100\t0.800\t0.100\t0.000\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.500\t0.500\t0.000\t0.000\n",
      "\n",
      "Emission matrix B:\n",
      "\t1\t2\t3\n",
      "C\t0.700\t0.200\t0.100\n",
      "H\t0.100\t0.200\t0.700\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.000\t0.000\t0.000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 274.19it/s]\n",
      "INFO : *** Compare to icdev corpus:\n",
      "2/H 3/H 3/H 2/H 3/H 2/H 3/H 2/H 2/H 3/H 1/H 3/H 3/H 1/C 1/C 1/C 2/C 1/C 1/C 1/C 3/C 1/C 2/C 1/C 1/C 1/C 2/H 3/H 3/H 2/H 3/H 2/H 2/H\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/H 3/H 3/H 2/H 3/H 2/H 3/H 2/H 2/H 3/H 1/H 3/H 3/H 1/C 1/C 1/C 2/C 1/C 1/C 1/C 3/C 1/C 2/C 1/C 1/C 1/C 2/H 3/H 3/H 2/H 3/H 2/H 2/H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 284.32it/s]\n",
      "INFO : Tagging accuracy: all: 100.000%, seen: 100.000%, novel: nan%\n",
      "INFO : *** A, B matrices as randomly initialized close to uniform\n",
      "INFO : *** Supervised HMM training on icsup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix A:\n",
      "\t(C|...)\t(H|...)\t(_EOS_TAG_|...)\t(_BOS_TAG_|...)\n",
      "C\t0.334\t0.334\t0.332\t0.000\n",
      "H\t0.334\t0.332\t0.334\t0.000\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.333\t0.334\t0.334\t0.000\n",
      "\n",
      "Emission matrix B:\n",
      "\t1\t2\t3\n",
      "C\t0.333\t0.335\t0.332\n",
      "H\t0.333\t0.333\t0.334\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.000\t0.000\t0.000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 1377.44it/s]\n",
      "INFO : Cross-entropy: 2.0979 nats (= perplexity 8.149)\n",
      "100%|██████████| 4/4 [00:00<00:00, 305.44it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1679.91it/s]\n",
      "INFO : Cross-entropy: 1.3729 nats (= perplexity 3.947)\n",
      "100%|██████████| 4/4 [00:00<00:00, 783.10it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1480.39it/s]\n",
      "INFO : Cross-entropy: 1.3729 nats (= perplexity 3.947)\n",
      "INFO : Saved model to my_hmm.pkl\n",
      "INFO : *** A, B matrices after training on icsup (should match initial params on spreadsheet [transposed])\n",
      "INFO : *** Viterbi results on icraw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix A:\n",
      "\t(C|...)\t(H|...)\t(_EOS_TAG_|...)\t(_BOS_TAG_|...)\n",
      "C\t0.800\t0.100\t0.100\t0.000\n",
      "H\t0.100\t0.800\t0.100\t0.000\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.500\t0.500\t0.000\t0.000\n",
      "\n",
      "Emission matrix B:\n",
      "\t1\t2\t3\n",
      "C\t0.700\t0.200\t0.100\n",
      "H\t0.100\t0.200\t0.700\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.000\t0.000\t0.000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.93it/s]\n",
      "INFO : *** Forward algorithm on icraw (should approximately match iteration 0 on spreadsheet)\n",
      "INFO : 9.12758979993639e-19 = p(2 3 3 2 3 2 3 2 2 3 1 3 3 1 1 1 2 1 1 1 3 1 2 1 1 1 2 3 3 2 3 2 2)\n",
      "INFO : *** Reestimating on icraw (perplexity should improve on every iteration)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/H 3/H 3/H 2/H 3/H 2/H 3/H 2/H 2/H 3/H 1/H 3/H 3/H 1/C 1/C 1/C 2/C 1/C 1/C 1/C 3/C 1/C 2/C 1/C 1/C 1/C 2/C 3/H 3/H 2/H 3/H 2/H 2/H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 494.61it/s]\n",
      "INFO : Cross-entropy: 1.2217 nats (= perplexity 3.393)\n",
      "100%|██████████| 1/1 [00:00<00:00, 123.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 183.72it/s]\n",
      "INFO : Cross-entropy: 1.0839 nats (= perplexity 2.956)\n",
      "100%|██████████| 1/1 [00:00<00:00, 76.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 606.03it/s]\n",
      "INFO : Cross-entropy: 1.0739 nats (= perplexity 2.927)\n",
      "100%|██████████| 1/1 [00:00<00:00, 223.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 701.86it/s]\n",
      "INFO : Cross-entropy: 1.0919 nats (= perplexity 2.980)\n",
      "INFO : Saved model to my_hmm.pkl\n",
      "INFO : *** A, B matrices after reestimation on icraw should match final params on spreadsheet [transposed])\n",
      "INFO : *** Viterbi results on icraw after reestimation on icraw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix A:\n",
      "\t(C|...)\t(H|...)\t(_EOS_TAG_|...)\t(_BOS_TAG_|...)\n",
      "C\t0.794\t0.206\t0.000\t0.000\n",
      "H\t0.028\t0.927\t0.045\t0.000\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.001\t0.999\t0.000\t0.000\n",
      "\n",
      "Emission matrix B:\n",
      "\t1\t2\t3\n",
      "C\t0.695\t0.155\t0.149\n",
      "H\t0.032\t0.482\t0.487\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.000\t0.000\t0.000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 407.17it/s]\n",
      "INFO : *** Conditional Random Field (CRF) test\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/H 3/H 3/H 2/H 3/H 2/H 3/H 2/H 2/H 3/H 1/C 3/C 3/C 1/C 1/C 1/C 2/C 1/C 1/C 1/C 3/C 1/C 2/C 1/C 1/C 1/C 2/H 3/H 3/H 2/H 3/H 2/H 2/H\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../code\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtest_ic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m negative_log_likelihood\n\u001b[1;32m      6\u001b[0m corpus \u001b[38;5;241m=\u001b[39m icraw\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# for i in range(10):\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#     for sentence in tqdm(corpus, total=len(corpus), leave=True):\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#         isent = hmm._integerize_sentence(sentence, corpus)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#         hmm.E_step(isent)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#         hmm.M_step(isent)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/JHU/Courses/NLP/NLP-JHU/hw-tag/data/../code/test_ic.py:147\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Now let's try out a randomly initialized CRF on the ice cream data. Notice how\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# the initialized A and B matrices now hold non-negative potentials,\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# rather than probabilities that sum to 1.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*** Conditional Random Field (CRF) test\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m crf \u001b[38;5;241m=\u001b[39m \u001b[43mConditionalRandomField\u001b[49m\u001b[43m(\u001b[49m\u001b[43micsup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43micsup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*** Current A, B matrices (potentials from small random parameters)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m crf\u001b[38;5;241m.\u001b[39mprintAB()\n",
      "File \u001b[0;32m~/Documents/JHU/Courses/NLP/NLP-JHU/hw-tag/code/crf.py:55\u001b[0m, in \u001b[0;36mConditionalRandomField.__init__\u001b[0;34m(self, tagset, vocab, unigram)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m     48\u001b[0m              tagset: Integerizer[Tag],\n\u001b[1;32m     49\u001b[0m              vocab: Integerizer[Word],\n\u001b[1;32m     50\u001b[0m              unigram: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct an CRF with initially random parameters, with the\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    given tagset, vocabulary, and lexical features.  See the super()\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    method for discussion.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munigram\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/JHU/Courses/NLP/NLP-JHU/hw-tag/data/../code/hmm.py:102\u001b[0m, in \u001b[0;36mHiddenMarkovModel.__init__\u001b[0;34m(self, tagset, vocab, unigram)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meos_t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# we need this to exist\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meye: Tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk\n\u001b[1;32m    100\u001b[0m )  \u001b[38;5;66;03m# identity matrix, used as a collection of one-hot tag vectors\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/JHU/Courses/NLP/NLP-JHU/hw-tag/code/crf.py:68\u001b[0m, in \u001b[0;36mConditionalRandomField.init_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize params self.WA and self.WB to small random values, and\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03mthen compute the potential matrices A, B from them.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03mAs in the parent method, we respect structural zeroes (\"Don't guess when you know\").\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# See the \"Training CRFs\" section of the reading handout.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# For a unigram model, self.WA should just have a single row:\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# that model has fewer parameters.\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m   \u001b[38;5;66;03m# you fill this in!\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdateAB()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "from test_ic.py import negative_log_likelihood\n",
    "\n",
    "corpus = icraw\n",
    "\n",
    "# for i in range(10):\n",
    "#     for sentence in tqdm(corpus, total=len(corpus), leave=True):\n",
    "#         isent = hmm._integerize_sentence(sentence, corpus)\n",
    "#         hmm.E_step(isent)\n",
    "#         hmm.M_step(isent)\n",
    "\n",
    "log.info(\"*** Viterbi results on icraw with hard coded parameters\")\n",
    "icraw = TaggedCorpus(Path(\"icraw\"), tagset=icsup.tagset, vocab=icsup.vocab)\n",
    "write_tagging(\n",
    "    hmm, icraw, Path(\"icraw_hmm.output\"), tagger_method=\"posterior\"\n",
    ")  # calls hmm.viterbi_tagging on each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'icsup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m icraw \u001b[38;5;241m=\u001b[39m TaggedCorpus(Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124micraw\u001b[39m\u001b[38;5;124m\"\u001b[39m), tagset\u001b[38;5;241m=\u001b[39m\u001b[43micsup\u001b[49m\u001b[38;5;241m.\u001b[39mtagset, vocab\u001b[38;5;241m=\u001b[39micsup\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[1;32m      2\u001b[0m write_tagging(\n\u001b[1;32m      3\u001b[0m     hmm, icraw, Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124micraw_hmm.output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m ) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'icsup' is not defined"
     ]
    }
   ],
   "source": [
    "icraw = TaggedCorpus(Path(\"icraw\"), tagset=icsup.tagset, vocab=icsup.vocab)\n",
    "write_tagging(\n",
    "    hmm, icraw, Path(\"icraw_hmm.output\")\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 40 tokens from icsup\n",
      "INFO : Created 4 tag types\n",
      "INFO : Created 5 word types\n",
      "INFO : Ice cream vocabulary: ['1', '2', '3', '_EOS_WORD_', '_BOS_WORD_']\n",
      "INFO : Ice cream tagset: ['C', 'H', '_EOS_TAG_', '_BOS_TAG_']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 1/C 1/C 1/C 1/C 1/C 1/C 1/C 2/C 2/C 3/H\n",
      "1/H 2/H 2/H 3/H 3/H 3/H 3/H 3/H 3/H 3/C\n",
      "1/C 1/C 1/C 1/C 1/C 1/C 1/C 2/C 2/C 3/H\n",
      "1/H 2/H 2/H 3/H 3/H 3/H 3/H 3/H 3/H 3/C\n",
      "INFO : *** Hidden Markov Model (HMM) test\n",
      "\n",
      "INFO : *** Current A, B matrices (using initalizations from the ice cream spreadsheet)\n",
      "INFO : *** Viterbi results on icraw with hard coded parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/C 1/C 1/C 1/C 1/C 1/C 1/C 2/C 2/C 3/H\n",
      "1/H 2/H 2/H 3/H 3/H 3/H 3/H 3/H 3/H 3/C\n",
      "1/C 1/C 1/C 1/C 1/C 1/C 1/C 2/C 2/C 3/H\n",
      "1/H 2/H 2/H 3/H 3/H 3/H 3/H 3/H 3/H 3/C\n",
      "Transition matrix A:\n",
      "\t(C|...)\t(H|...)\t(_EOS_TAG_|...)\t(_BOS_TAG_|...)\n",
      "C\t0.800\t0.100\t0.100\t0.000\n",
      "H\t0.100\t0.800\t0.100\t0.000\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.500\t0.500\t0.000\t0.000\n",
      "\n",
      "Emission matrix B:\n",
      "\t1\t2\t3\n",
      "C\t0.700\t0.200\t0.100\n",
      "H\t0.100\t0.200\t0.700\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.000\t0.000\t0.000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 344.81it/s]\n",
      "INFO : *** Compare to icdev corpus:\n",
      "2/H 3/H 3/H 2/H 3/H 2/H 3/H 2/H 2/H 3/H 1/H 3/H 3/H 1/C 1/C 1/C 2/C 1/C 1/C 1/C 3/C 1/C 2/C 1/C 1/C 1/C 2/H 3/H 3/H 2/H 3/H 2/H 2/H\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/H 3/H 3/H 2/H 3/H 2/H 3/H 2/H 2/H 3/H 1/H 3/H 3/H 1/C 1/C 1/C 2/C 1/C 1/C 1/C 3/C 1/C 2/C 1/C 1/C 1/C 2/H 3/H 3/H 2/H 3/H 2/H 2/H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 210.89it/s]\n",
      "INFO : Tagging accuracy: all: 100.000%, seen: 100.000%, novel: nan%\n",
      "INFO : *** A, B matrices as randomly initialized close to uniform\n",
      "INFO : *** Supervised HMM training on icsup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix A:\n",
      "\t(C|...)\t(H|...)\t(_EOS_TAG_|...)\t(_BOS_TAG_|...)\n",
      "C\t0.334\t0.333\t0.333\t0.000\n",
      "H\t0.334\t0.334\t0.332\t0.000\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.332\t0.335\t0.333\t0.000\n",
      "\n",
      "Emission matrix B:\n",
      "\t1\t2\t3\n",
      "C\t0.333\t0.333\t0.335\n",
      "H\t0.334\t0.333\t0.333\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.000\t0.000\t0.000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 1515.15it/s]\n",
      "INFO : Cross-entropy: 2.0974 nats (= perplexity 8.145)\n",
      "100%|██████████| 4/4 [00:00<00:00, 1320.21it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 2140.22it/s]\n",
      "INFO : Cross-entropy: 1.3729 nats (= perplexity 3.947)\n",
      "100%|██████████| 4/4 [00:00<00:00, 2544.70it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1953.79it/s]\n",
      "INFO : Cross-entropy: 1.3729 nats (= perplexity 3.947)\n",
      "INFO : Saved model to my_hmm.pkl\n",
      "INFO : *** A, B matrices after training on icsup (should match initial params on spreadsheet [transposed])\n",
      "INFO : *** Viterbi results on icraw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix A:\n",
      "\t(C|...)\t(H|...)\t(_EOS_TAG_|...)\t(_BOS_TAG_|...)\n",
      "C\t0.800\t0.100\t0.100\t0.000\n",
      "H\t0.100\t0.800\t0.100\t0.000\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.500\t0.500\t0.000\t0.000\n",
      "\n",
      "Emission matrix B:\n",
      "\t1\t2\t3\n",
      "C\t0.700\t0.200\t0.100\n",
      "H\t0.100\t0.200\t0.700\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.000\t0.000\t0.000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 334.47it/s]\n",
      "INFO : *** Forward algorithm on icraw (should approximately match iteration 0 on spreadsheet)\n",
      "INFO : 9.127694257509654e-19 = p(2 3 3 2 3 2 3 2 2 3 1 3 3 1 1 1 2 1 1 1 3 1 2 1 1 1 2 3 3 2 3 2 2)\n",
      "INFO : *** Reestimating on icraw (perplexity should improve on every iteration)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/H 3/H 3/H 2/H 3/H 2/H 3/H 2/H 2/H 3/H 1/H 3/H 3/H 1/C 1/C 1/C 2/C 1/C 1/C 1/C 3/C 1/C 2/C 1/C 1/C 1/C 2/H 3/H 3/H 2/H 3/H 2/H 2/H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 565.57it/s]\n",
      "INFO : Cross-entropy: 1.2217 nats (= perplexity 3.393)\n",
      "100%|██████████| 1/1 [00:00<00:00, 1453.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 680.34it/s]\n",
      "INFO : Cross-entropy: 1.4922 nats (= perplexity 4.447)\n",
      "INFO : Saved model to my_hmm.pkl\n",
      "INFO : *** A, B matrices after reestimation on icraw should match final params on spreadsheet [transposed])\n",
      "INFO : *** Viterbi results on icraw after reestimation on icraw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix A:\n",
      "\t(C|...)\t(H|...)\t(_EOS_TAG_|...)\t(_BOS_TAG_|...)\n",
      "C\t0.333\t0.333\t0.333\t0.000\n",
      "H\t0.333\t0.333\t0.333\t0.000\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.333\t0.333\t0.333\t0.000\n",
      "\n",
      "Emission matrix B:\n",
      "\t1\t2\t3\n",
      "C\t0.333\t0.333\t0.333\n",
      "H\t0.333\t0.333\t0.333\n",
      "_EOS_TAG_\t0.000\t0.000\t0.000\n",
      "_BOS_TAG_\t0.000\t0.000\t0.000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 369.71it/s]\n",
      "INFO : *** Conditional Random Field (CRF) test\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/C 3/C 3/C 2/C 3/C 2/C 3/C 2/C 2/C 3/C 1/C 3/C 3/C 1/C 1/C 1/C 2/C 1/C 1/C 1/C 3/C 1/C 2/C 1/C 1/C 1/C 2/C 3/C 3/C 2/C 3/C 2/C 2/C\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../code\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtest_ic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m negative_log_likelihood\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m tqdm(corpus, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(corpus), leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m      5\u001b[0m     isent \u001b[38;5;241m=\u001b[39m hmm\u001b[38;5;241m.\u001b[39m_integerize_sentence(sentence, corpus)\n",
      "File \u001b[0;32m~/Documents/JHU/Courses/NLP/NLP-JHU/hw-tag/data/../code/test_ic.py:147\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Now let's try out a randomly initialized CRF on the ice cream data. Notice how\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# the initialized A and B matrices now hold non-negative potentials,\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# rather than probabilities that sum to 1.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*** Conditional Random Field (CRF) test\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 147\u001b[0m crf \u001b[38;5;241m=\u001b[39m \u001b[43mConditionalRandomField\u001b[49m\u001b[43m(\u001b[49m\u001b[43micsup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43micsup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*** Current A, B matrices (potentials from small random parameters)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m crf\u001b[38;5;241m.\u001b[39mprintAB()\n",
      "File \u001b[0;32m~/Documents/JHU/Courses/NLP/NLP-JHU/hw-tag/code/crf.py:55\u001b[0m, in \u001b[0;36mConditionalRandomField.__init__\u001b[0;34m(self, tagset, vocab, unigram)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m     48\u001b[0m              tagset: Integerizer[Tag],\n\u001b[1;32m     49\u001b[0m              vocab: Integerizer[Word],\n\u001b[1;32m     50\u001b[0m              unigram: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct an CRF with initially random parameters, with the\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    given tagset, vocabulary, and lexical features.  See the super()\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    method for discussion.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munigram\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/JHU/Courses/NLP/NLP-JHU/hw-tag/code/hmm.py:102\u001b[0m, in \u001b[0;36mHiddenMarkovModel.__init__\u001b[0;34m(self, tagset, vocab, unigram)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meos_t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# we need this to exist\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meye: Tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk\n\u001b[1;32m    100\u001b[0m )  \u001b[38;5;66;03m# identity matrix, used as a collection of one-hot tag vectors\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/JHU/Courses/NLP/NLP-JHU/hw-tag/code/crf.py:68\u001b[0m, in \u001b[0;36mConditionalRandomField.init_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize params self.WA and self.WB to small random values, and\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03mthen compute the potential matrices A, B from them.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03mAs in the parent method, we respect structural zeroes (\"Don't guess when you know\").\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# See the \"Training CRFs\" section of the reading handout.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# For a unigram model, self.WA should just have a single row:\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# that model has fewer parameters.\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m   \u001b[38;5;66;03m# you fill this in!\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdateAB()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sys.path.append(\"../code\")\n",
    "from test_ic.py import negative_log_likelihood\n",
    "\n",
    "for sentence in tqdm(corpus, total=len(corpus), leave=True):\n",
    "    isent = hmm._integerize_sentence(sentence, corpus)\n",
    "    hmm.E_step(isent=isent)\n",
    "    hmm.M_step(isent=isent)\n",
    "    hmm.printAB()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hmm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhmm\u001b[49m\u001b[38;5;241m.\u001b[39mprintAB()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hmm' is not defined"
     ]
    }
   ],
   "source": [
    "hmm.printAB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : *** Hidden Markov Model (HMM)\n",
      "100%|██████████| 4051/4051 [00:13<00:00, 310.45it/s]\n",
      "INFO : Cross-entropy: 12.6445 nats (= perplexity 310045.271)\n",
      "100%|██████████| 4051/4051 [00:31<00:00, 129.50it/s]\n",
      "100%|██████████| 4051/4051 [00:09<00:00, 422.11it/s]\n",
      "INFO : Cross-entropy: 7.4503 nats (= perplexity 1720.395)\n",
      "100%|██████████| 4051/4051 [00:29<00:00, 138.85it/s]\n",
      "100%|██████████| 4051/4051 [00:09<00:00, 416.39it/s]\n",
      "INFO : Cross-entropy: 7.4503 nats (= perplexity 1720.403)\n",
      "INFO : Saved model to ensup_hmm.pkl\n"
     ]
    }
   ],
   "source": [
    "log.info(\"*** Hidden Markov Model (HMM)\")\n",
    "hmm = HiddenMarkovModel(entrain.tagset, entrain.vocab)  # randomly initialized parameters  \n",
    "loss_sup = lambda model: model_cross_entropy(model, eval_corpus=ensup)\n",
    "hmm.train(corpus=ensup, loss=loss_sup, λ=1.0,\n",
    "          save_path=\"ensup_hmm.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's throw in the unsupervised training data as well, and continue\n",
    "training as before, in order to increase the regularized log-likelihood on\n",
    "this larger, semi-supervised training set.  It's now the *incomplete-data*\n",
    "log-likelihood.\n",
    "\n",
    "This time, we'll use a different evaluation loss function: we'll stop when the\n",
    "*tagging error rate* on a held-out dev set stops getting better.  Also, the\n",
    "implementation of this loss function (`viterbi_error_rate`) includes a helpful\n",
    "side effect: it logs the *cross-entropy* on the held-out dataset as well, just\n",
    "for your information.\n",
    "\n",
    "We hope that held-out tagging accuracy will go up for a little bit before it\n",
    "goes down again (see Merialdo 1994). (Log-likelihood on training data will\n",
    "continue to improve, and that improvement may generalize to held-out\n",
    "cross-entropy.  But getting accuracy to increase is harder.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Loaded model from ensup_hmm.pkl\n",
      "100%|██████████| 996/996 [00:03<00:00, 313.12it/s]\n",
      "INFO : Cross-entropy: 7.5993 nats (= perplexity 1996.797)\n",
      "100%|██████████| 996/996 [00:05<00:00, 175.49it/s]\n",
      "INFO : Tagging accuracy: all: 88.663%, known: 93.059%, seen: 44.108%, novel: 42.734%\n",
      "100%|██████████| 8064/8064 [01:15<00:00, 106.45it/s]\n",
      "100%|██████████| 996/996 [00:03<00:00, 320.06it/s]\n",
      "INFO : Cross-entropy: 7.3485 nats (= perplexity 1553.848)\n",
      "100%|██████████| 996/996 [00:14<00:00, 66.50it/s] \n",
      "INFO : Tagging accuracy: all: 87.031%, known: 91.397%, seen: 45.791%, novel: 40.225%\n",
      "INFO : Saved model to entrain_hmm.pkl\n"
     ]
    }
   ],
   "source": [
    "hmm = HiddenMarkovModel.load(\"ensup_hmm.pkl\")  # reset to supervised model (in case you're re-executing this bit)\n",
    "loss_dev = lambda model: viterbi_error_rate(model, eval_corpus=endev, \n",
    "                                            known_vocab=known_vocab)\n",
    "hmm.train(corpus=entrain, loss=loss_dev, λ=1.0,\n",
    "          save_path=\"entrain_hmm.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also retry the above workflow where you start with a worse supervised\n",
    "model (like Merialdo).  Does EM help more in that case?  It's easiest to rerun\n",
    "exactly the code above, but first make the `ensup` file smaller by copying\n",
    "`ensup-tiny` over it.  `ensup-tiny` is only 25 sentences (that happen to cover\n",
    "all tags in `endev`).  Back up your old `ensup` and your old `*.pkl` models\n",
    "before you do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More detailed look at the first 10 sentences in the held-out corpus,\n",
    "including Viterbi tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of corpus failed: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/nlp-class-tag/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/nlp-class-tag/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/nlp-class-tag/lib/python3.9/importlib/__init__.py\", line 168, in reload\n",
      "    raise ModuleNotFoundError(f\"spec not found for the module {name!r}\", name=name)\n",
      "ModuleNotFoundError: spec not found for the module 'corpus'\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "def look_at_your_data(model, dev, N):\n",
    "    for m, sentence in enumerate(dev):\n",
    "        if m >= N: break\n",
    "        viterbi = model.viterbi_tagging(sentence.desupervise(), endev)\n",
    "        counts = eval_tagging(predicted=viterbi, gold=sentence, \n",
    "                              known_vocab=known_vocab)\n",
    "        num = counts['NUM', 'ALL']\n",
    "        denom = counts['DENOM', 'ALL']\n",
    "        \n",
    "        log.info(f\"Gold:    {sentence}\")\n",
    "        log.info(f\"Viterbi: {viterbi}\")\n",
    "        log.info(f\"Loss:    {denom - num}/{denom}\")\n",
    "        xent = -model.logprob(sentence, endev) / len(sentence)  # measured in nats\n",
    "        log.info(f\"Cross-entropy: {xent/math.log(2)} nats (= perplexity {math.exp(xent)})\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Gold:    ``/` We/P 're/V strongly/R _OOV_/V that/I anyone/N who/W has/V eaten/V in/I the/D cafeteria/N this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/N added/V ,/, ``/` and/C that/D means/V virtually/R everyone/N who/W works/V here/R ./.\n",
      "INFO : Viterbi: ``/` We/P 're/V strongly/R _OOV_/V that/I anyone/N who/W has/V eaten/V in/I the/D cafeteria/N this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/T added/V ,/, ``/` and/C that/I means/V virtually/R everyone/, who/W works/V here/R ./.\n",
      "INFO : Loss:    3/34\n",
      "INFO : Cross-entropy: 10.61779499053955 nats (= perplexity 1571.3564102110174)\n",
      "---\n",
      "INFO : Gold:    I/P was/V _OOV_/V to/T read/V the/D _OOV_/N of/I facts/N in/I your/P Oct./N 13/C editorial/N ``/` _OOV_/N 's/P _OOV_/N _OOV_/N ./. ''/'\n",
      "INFO : Viterbi: I/P was/V _OOV_/V to/T read/V the/D _OOV_/N of/I facts/N in/I your/P Oct./N 13/C editorial/, ``/` _OOV_/P 's/V _OOV_/D _OOV_/N ./. ''/'\n",
      "INFO : Loss:    4/21\n",
      "INFO : Cross-entropy: 10.876285552978516 nats (= perplexity 1879.69911304983)\n",
      "---\n",
      "INFO : Gold:    It/P is/V the/D _OOV_/J guerrillas/N who/W are/V aligned/V with/I the/D drug/N traffickers/N ,/, not/R the/D left/J _OOV_/N ./.\n",
      "INFO : Viterbi: It/P is/V the/D _OOV_/N guerrillas/, who/W are/V aligned/R with/I the/D drug/N traffickers/N ,/, not/R the/D left/J _OOV_/N ./.\n",
      "INFO : Loss:    3/18\n",
      "INFO : Cross-entropy: 9.650657653808594 nats (= perplexity 803.7805517322801)\n",
      "---\n",
      "INFO : Gold:    This/D information/N was/V _OOV_/V from/I your/P own/J news/N stories/N on/I the/D region/N ./.\n",
      "INFO : Viterbi: This/D information/N was/V _OOV_/R from/I your/P own/J news/N stories/N on/I the/D region/N ./.\n",
      "INFO : Loss:    1/13\n",
      "INFO : Cross-entropy: 9.34328556060791 nats (= perplexity 649.5449924622674)\n",
      "---\n",
      "INFO : Gold:    _OOV_/J _OOV_/J government/N _OOV_/N of/I the/D ``/` _OOV_/F ''/' was/V due/J to/T the/D drug/N _OOV_/N '/P history/N of/I _OOV_/V out/R _OOV_/N in/I the/D _OOV_/N ./.\n",
      "INFO : Viterbi: _OOV_/D _OOV_/J government/N _OOV_/N of/I the/D ``/N _OOV_/, ''/' was/V due/J to/T the/D drug/N _OOV_/I '/P history/N of/I _OOV_/N out/I _OOV_/N in/I the/D _OOV_/N ./.\n",
      "INFO : Loss:    6/25\n",
      "INFO : Cross-entropy: 10.975038528442383 nats (= perplexity 2012.870256639555)\n",
      "---\n",
      "INFO : Gold:    Mary/N _OOV_/N Palo/N Alto/N ,/, Calif/N ./.\n",
      "INFO : Viterbi: Mary/N _OOV_/I Palo/D Alto/N ,/, Calif/N ./.\n",
      "INFO : Loss:    2/7\n",
      "INFO : Cross-entropy: 10.526596069335938 nats (= perplexity 1475.0993990210206)\n",
      "---\n",
      "INFO : Gold:    I/P suggest/V that/I The/D Wall/N Street/N Journal/N -LRB-/- as/R well/R as/I other/J U.S./N news/N publications/N of/I like/J mind/N -RRB-/- should/M put/V its/P money/N where/W its/P mouth/N is/V :/: _OOV_/V computer/N equipment/N to/T replace/V that/I damaged/V at/I El/N _OOV_/N ,/, buy/V ad/N space/N ,/, publish/V stories/N under/I the/D _OOV_/N of/I El/N _OOV_/N journalists/N ./.\n",
      "INFO : Viterbi: I/P suggest/V that/I The/D Wall/N Street/N Journal/N -LRB-/- as/I well/R as/I other/J U.S./N news/N publications/N of/I like/I mind/N -RRB-/N should/M put/V its/P money/N where/W its/P mouth/M is/V :/I _OOV_/D computer/N equipment/N to/T replace/V that/I damaged/N at/I El/D _OOV_/N ,/, buy/V ad/N space/N ,/, publish/V stories/N under/I the/D _OOV_/N of/I El/D _OOV_/J journalists/N ./.\n",
      "INFO : Loss:    10/53\n",
      "INFO : Cross-entropy: 11.649870872497559 nats (= perplexity 3213.368842789746)\n",
      "---\n",
      "INFO : Gold:    Perhaps/R an/D arrangement/N could/M be/V worked/V out/R to/T ``/` sponsor/V ''/' El/N _OOV_/N journalists/N and/C staff/N by/I paying/V for/I added/V security/N in/I exchange/N for/I exclusive/J stories/N ./.\n",
      "INFO : Viterbi: Perhaps/I an/D arrangement/N could/M be/V worked/V out/R to/T ``/` sponsor/F ''/' El/V _OOV_/T journalists/$ and/C staff/N by/I paying/V for/I added/J security/N in/I exchange/N for/I exclusive/J stories/N ./.\n",
      "INFO : Loss:    6/27\n",
      "INFO : Cross-entropy: 11.456055641174316 nats (= perplexity 2809.4189341157016)\n",
      "---\n",
      "INFO : Gold:    _OOV_/V El/N _OOV_/N 's/P courage/N with/I real/J support/N ./.\n",
      "INFO : Viterbi: _OOV_/D El/N _OOV_/I 's/P courage/N with/I real/J support/N ./.\n",
      "INFO : Loss:    2/9\n",
      "INFO : Cross-entropy: 10.91425609588623 nats (= perplexity 1929.8268191741217)\n",
      "---\n",
      "INFO : Gold:    Douglas/N B./N Evans/N\n",
      "INFO : Viterbi: Douglas/D B./N Evans/.\n",
      "INFO : Loss:    2/3\n",
      "INFO : Cross-entropy: 11.670408248901367 nats (= perplexity 3259.4398878531224)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "look_at_your_data(hmm, endev, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try supervised training of a CRF (this doesn't use the unsupervised\n",
    "part of the data, so it is comparable to the supervised pre-training we did\n",
    "for the HMM).  We will use SGD to approximately maximize the regularized\n",
    "log-likelihood. \n",
    "\n",
    "As with the semi-supervised HMM training, we'll periodically evaluate the\n",
    "tagging accuracy (and also print the cross-entropy) on a held-out dev set.\n",
    "We use the default `eval_interval` and `tolerance`.  If you want to stop\n",
    "sooner, then you could increase the `tolerance` so the training method decides\n",
    "sooner that it has converged.\n",
    "\n",
    "We arbitrarily choose reg = 1.0 for L2 regularization, learning rate = 0.05,\n",
    "and a minibatch size of 10, but it would be better to search for the best\n",
    "value of these hyperparameters.\n",
    "\n",
    "Note that the logger reports the CRF's *conditional* cross-entropy, log p(tags\n",
    "| words) / n.  This is much lower than the HMM's *joint* cross-entropy log\n",
    "p(tags, words) / n, but that doesn't mean the CRF is worse at tagging.  The\n",
    "CRF is just predicting less information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : *** Conditional Random Field (CRF)\n",
      "\n",
      "100%|██████████| 996/996 [00:07<00:00, 140.96it/s]\n",
      "INFO : Cross-entropy: 3.0510 nats (= perplexity 21.136)\n",
      "100%|██████████| 996/996 [00:04<00:00, 228.30it/s]\n",
      "INFO : Tagging accuracy: all: 4.514%, known: 4.720%, seen: 5.219%, novel: 1.255%\n",
      "100%|██████████| 500/500 [00:08<00:00, 60.60it/s]\n",
      "100%|██████████| 996/996 [00:06<00:00, 156.50it/s]\n",
      "INFO : Cross-entropy: 0.9115 nats (= perplexity 2.488)\n",
      "100%|██████████| 996/996 [00:07<00:00, 137.38it/s]\n",
      "INFO : Tagging accuracy: all: 72.396%, known: 73.582%, seen: 57.912%, novel: 60.964%\n",
      "100%|██████████| 500/500 [00:10<00:00, 45.88it/s]\n",
      "100%|██████████| 996/996 [00:06<00:00, 164.46it/s]\n",
      "INFO : Cross-entropy: 0.7517 nats (= perplexity 2.121)\n",
      "100%|██████████| 996/996 [00:05<00:00, 181.65it/s]\n",
      "INFO : Tagging accuracy: all: 75.247%, known: 77.016%, seen: 55.892%, novel: 57.332%\n",
      "100%|██████████| 500/500 [00:10<00:00, 46.62it/s]\n",
      "100%|██████████| 996/996 [00:06<00:00, 149.54it/s]\n",
      "INFO : Cross-entropy: 0.6582 nats (= perplexity 1.931)\n",
      "100%|██████████| 996/996 [00:05<00:00, 187.09it/s]\n",
      "INFO : Tagging accuracy: all: 78.763%, known: 80.321%, seen: 61.785%, novel: 62.946%\n",
      "100%|██████████| 500/500 [00:10<00:00, 45.66it/s]\n",
      "100%|██████████| 996/996 [00:06<00:00, 159.77it/s]\n",
      "INFO : Cross-entropy: 0.6044 nats (= perplexity 1.830)\n",
      "100%|██████████| 996/996 [00:05<00:00, 195.37it/s]\n",
      "INFO : Tagging accuracy: all: 80.575%, known: 82.176%, seen: 62.121%, novel: 64.729%\n",
      "100%|██████████| 500/500 [00:10<00:00, 47.19it/s]\n",
      "100%|██████████| 996/996 [00:05<00:00, 166.47it/s]\n",
      "INFO : Cross-entropy: 0.5692 nats (= perplexity 1.767)\n",
      "100%|██████████| 996/996 [00:05<00:00, 188.07it/s]\n",
      "INFO : Tagging accuracy: all: 80.429%, known: 81.883%, seen: 64.310%, novel: 65.786%\n",
      "100%|██████████| 500/500 [00:12<00:00, 38.66it/s]\n",
      "100%|██████████| 996/996 [00:07<00:00, 139.36it/s]\n",
      "INFO : Cross-entropy: 0.5370 nats (= perplexity 1.711)\n",
      "100%|██████████| 996/996 [00:05<00:00, 180.56it/s]\n",
      "INFO : Tagging accuracy: all: 81.561%, known: 83.160%, seen: 64.141%, novel: 65.324%\n",
      "100%|██████████| 500/500 [00:22<00:00, 22.23it/s]\n",
      "100%|██████████| 996/996 [00:06<00:00, 152.97it/s]\n",
      "INFO : Cross-entropy: 0.5110 nats (= perplexity 1.667)\n",
      "100%|██████████| 996/996 [00:06<00:00, 165.86it/s]\n",
      "INFO : Tagging accuracy: all: 83.615%, known: 85.390%, seen: 63.131%, novel: 66.050%\n",
      "100%|██████████| 500/500 [00:11<00:00, 44.34it/s]\n",
      "100%|██████████| 996/996 [00:04<00:00, 210.99it/s]\n",
      "INFO : Cross-entropy: 0.4888 nats (= perplexity 1.630)\n",
      "100%|██████████| 996/996 [00:04<00:00, 238.89it/s]\n",
      "INFO : Tagging accuracy: all: 84.225%, known: 86.113%, seen: 63.131%, novel: 65.258%\n",
      "100%|██████████| 500/500 [00:08<00:00, 56.04it/s]\n",
      "100%|██████████| 996/996 [00:05<00:00, 191.02it/s]\n",
      "INFO : Cross-entropy: 0.4724 nats (= perplexity 1.604)\n",
      "100%|██████████| 996/996 [00:03<00:00, 258.85it/s]\n",
      "INFO : Tagging accuracy: all: 84.667%, known: 86.599%, seen: 62.963%, novel: 65.324%\n",
      "100%|██████████| 500/500 [00:07<00:00, 64.12it/s]\n",
      "100%|██████████| 996/996 [00:04<00:00, 220.42it/s]\n",
      "INFO : Cross-entropy: 0.4588 nats (= perplexity 1.582)\n",
      "100%|██████████| 996/996 [00:03<00:00, 265.93it/s]\n",
      "INFO : Tagging accuracy: all: 84.893%, known: 86.942%, seen: 62.458%, novel: 64.135%\n",
      "100%|██████████| 500/500 [00:07<00:00, 62.95it/s]\n",
      "100%|██████████| 996/996 [00:04<00:00, 218.04it/s]\n",
      "INFO : Cross-entropy: 0.4443 nats (= perplexity 1.559)\n",
      "100%|██████████| 996/996 [00:03<00:00, 261.69it/s]\n",
      "INFO : Tagging accuracy: all: 85.235%, known: 87.230%, seen: 62.795%, novel: 65.258%\n",
      "100%|██████████| 500/500 [00:07<00:00, 65.02it/s]\n",
      "100%|██████████| 996/996 [00:04<00:00, 226.80it/s]\n",
      "INFO : Cross-entropy: 0.4346 nats (= perplexity 1.544)\n",
      "100%|██████████| 996/996 [00:03<00:00, 276.41it/s]\n",
      "INFO : Tagging accuracy: all: 85.482%, known: 87.432%, seen: 63.636%, novel: 65.918%\n",
      "100%|██████████| 500/500 [00:08<00:00, 62.49it/s]\n",
      "100%|██████████| 996/996 [00:04<00:00, 227.49it/s]\n",
      "INFO : Cross-entropy: 0.4226 nats (= perplexity 1.526)\n",
      "100%|██████████| 996/996 [00:15<00:00, 65.55it/s] \n",
      "INFO : Tagging accuracy: all: 85.912%, known: 87.881%, seen: 63.636%, novel: 66.248%\n",
      " 33%|███▎      | 165/500 [00:02<00:05, 56.55it/s]INFO : Saved model to ensup_crf-6670.pkl\n",
      "100%|██████████| 500/500 [00:08<00:00, 61.79it/s]\n",
      "100%|██████████| 996/996 [00:04<00:00, 218.05it/s]\n",
      "INFO : Cross-entropy: 0.4163 nats (= perplexity 1.516)\n",
      "100%|██████████| 996/996 [00:03<00:00, 276.38it/s]\n",
      "INFO : Tagging accuracy: all: 85.903%, known: 87.835%, seen: 64.141%, novel: 66.579%\n",
      "INFO : Saved model to ensup_crf.pkl\n"
     ]
    }
   ],
   "source": [
    "log.info(\"*** Conditional Random Field (CRF)\\n\")\n",
    "crf = ConditionalRandomField(entrain.tagset, entrain.vocab)  # randomly initialized parameters  \n",
    "crf.train(corpus=ensup, loss=loss_dev, reg=1.0, lr=0.05, minibatch_size=10,\n",
    "          save_path=\"ensup_crf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine how the CRF does on individual sentences. \n",
    "(Do you see any error patterns here that would inspire additional CRF features?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_at_your_data(crf, endev, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "HMM trained on ensup and evaluated on endev - Lambda 0\n",
    "INFO:eval:Cross-entropy: 9.9119 nats (= perplexity 20169.835)\n",
    "INFO:eval:Tagging accuracy: all: 90.455%, known: 96.786%, seen: nan%, novel: 24.858%\n",
    "\n",
    "HMM trained on ensup and evaluated on endev - Lambda 0.5\n",
    "INFO:eval:Cross-entropy: 7.1537 nats (= perplexity 1278.872)\n",
    "INFO:eval:Tagging accuracy: all: 89.866%, known: 94.542%, seen: nan%, novel: 41.414%\n",
    "\n",
    "HMM trained on ensup and evaluated on endev - Lambda 1\n",
    "INFO:eval:Cross-entropy: 7.3767 nats (= perplexity 1598.299)\n",
    "INFO:eval:Tagging accuracy: all: 88.421%, known: 92.871%, seen: nan%, novel: 42.315%\n",
    "\n",
    "HMM trained on ensup and evaluated on endev - Lambda 2\n",
    "INFO:eval:Cross-entropy: 7.6768 nats (= perplexity 2157.697)\n",
    "INFO:eval:Tagging accuracy: all: 86.609%, known: 90.733%, seen: nan%, novel: 43.880%\n",
    "\n",
    "CRF trained on ensup and evaluated on endev - LR 0.05\n",
    "INFO:eval:Cross-entropy: 0.2564 nats (= perplexity 1.292)\n",
    "INFO:eval:Tagging accuracy: all: 91.031%, known: 93.682%, seen: nan%, novel: 63.567%\n",
    "\n",
    "CRF Batch Size 32 - LR 0.05\n",
    "INFO:eval:Cross-entropy: 0.2717 nats (= perplexity 1.312)\n",
    "INFO:eval:Tagging accuracy: all: 90.626%, known: 93.109%, seen: nan%, novel: 64.896%\n",
    "\n",
    "CRF Batch Size 32 - LR 0.01\n",
    "INFO:eval:Cross-entropy: 0.3600 nats (= perplexity 1.433)\n",
    "INFO:eval:Tagging accuracy: all: 87.482%, known: 89.666%, seen: nan%, novel: 64.848%\n",
    "\n",
    "CRF Batch Size 32 - LR 0.1\n",
    "INFO:eval:Cross-entropy: 0.5340 nats (= perplexity 1.706)\n",
    "INFO:eval:Tagging accuracy: all: 82.003%, known: 83.746%, seen: nan%, novel: 63.947%\n",
    "\n",
    "CRF Batch Size 128 - LR 0.05\n",
    "INFO:eval:Cross-entropy: 4.4629 nats (= perplexity 86.738)\n",
    "INFO:eval:Tagging accuracy: all: 61.623%, known: 62.126%, seen: nan%, novel: 56.404%\n",
    "\n",
    "CRF Batch Size 256 - LR 0.5\n",
    "INFO:eval:Cross-entropy: 7.6683 nats (= perplexity 2139.514)\n",
    "INFO:eval:Tagging accuracy: all: 63.318%, known: 66.471%, seen: nan%, novel: 30.645%\n",
    "\n",
    "CRF Batch Size 32 - LR 0.01 Reg 1e-4\n",
    "INFO:eval:Cross-entropy: 0.2716 nats (= perplexity 1.312)\n",
    "INFO:eval:Tagging accuracy: all: 90.643%, known: 93.128%, seen: nan%, novel: 64.896%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-class-tag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
