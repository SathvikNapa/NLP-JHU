{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cee7e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1642c07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2546bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpus import TaggedCorpus\n",
    "from eval import eval_tagging, model_cross_entropy, viterbi_error_rate\n",
    "from hmm import HiddenMarkovModel\n",
    "from crf import ConditionalRandomField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "872a2140",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.root.setLevel(level=logging.INFO)\n",
    "log = logging.getLogger(\"test_en\")       # For usage, see findsim.py in earlier assignment.\n",
    "logging.basicConfig(format=\"%(levelname)s : %(message)s\", level=logging.INFO)  # could change INFO to DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3840197e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 191873 tokens from ensup, enraw\n",
      "INFO : Created 26 tag types\n",
      "INFO : Created 18461 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(entrain)=8064  len(ensup)=4051  len(endev)=996 len(enraw)=4013\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"../data\")\n",
    "entrain = TaggedCorpus(Path(\"ensup\"), Path(\"enraw\"))                               # all training\n",
    "ensup =   TaggedCorpus(Path(\"ensup\"), tagset=entrain.tagset, vocab=entrain.vocab)  # supervised training\n",
    "endev =   TaggedCorpus(Path(\"endev\"), tagset=entrain.tagset, vocab=entrain.vocab)  # evaluation\n",
    "enraw =   TaggedCorpus(Path(\"enraw\"), tagset=entrain.tagset, vocab=entrain.vocab)  # raw\n",
    "print(f\"{len(entrain)=}  {len(ensup)=}  {len(endev)=} {len(enraw)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9619b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 283514 tokens from czsup, czraw\n",
      "INFO : Created 67 tag types\n",
      "INFO : Created 50542 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(cztrain)=16469  len(czsup)=8439  len(czdev)=2148 len(czraw)=8030\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"../data\")\n",
    "cztrain = TaggedCorpus(Path(\"czsup\"), Path(\"czraw\"))                               # all training\n",
    "czsup =   TaggedCorpus(Path(\"czsup\"), tagset=cztrain.tagset, vocab=cztrain.vocab)  # supervised training\n",
    "czdev =   TaggedCorpus(Path(\"czdev\"), tagset=cztrain.tagset, vocab=cztrain.vocab)  # evaluation\n",
    "czraw =   TaggedCorpus(Path(\"czraw\"), tagset=cztrain.tagset, vocab=cztrain.vocab)  # raw\n",
    "print(f\"{len(cztrain)=}  {len(czsup)=}  {len(czdev)=} {len(czraw)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190f24de",
   "metadata": {},
   "outputs": [],
   "source": [
    "* The perplexity of the hmm model trained on ensup data produces lower perplexity on enraw data than the endev data. The number of records observed in the endev dataset is too few to make a fair comparison against the enraw unlabeled data, which has 4x number of records.\n",
    "The perplexity of the model when evaluated on ensup4k is ~534 much lesser than the other two datasets, which might be due to the presence of similar tagged sentences in the evaluation dataset to what the model was trained on.\n",
    "\n",
    "* The perplexity of the hmm model trained on czsup data produces lower perpexity on the dev data than the czraw data.\n",
    "\n",
    "> During the evaluation of model on raw data, the score computation block marginalizes over all possible tags \n",
    "and hence it might result in lower perplexity.\n",
    "> During the evaluation of model on dev data, the score computation block scores on the joint distribution of word and tags, which might be higher.\n",
    "> For language modeling/ language generation, the perplexity on the raw data matters more.\n",
    "> For the supervised tagging task, the perplexity on the dev data matters more. But ultimately, the accuracy and F1 score give us a better picture than the perplexity on the dev data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57e2a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, importlib\n",
    "try:\n",
    "    get_ipython().run_line_magic(\"autoreload\", \"0\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Ensure we only have a single copy of these modules\n",
    "for m in [\"corpus\", \"hmm\", \"eval\", \"integerize\", \"tag\", \"crf\", \"crf_neural\", \"lexicon\"]:\n",
    "    if m in sys.modules:\n",
    "        del sys.modules[m]\n",
    "\n",
    "import corpus\n",
    "import hmm\n",
    "\n",
    "# Reload in this order so hmm binds to the SAME corpus.Sentence class\n",
    "importlib.reload(corpus)\n",
    "importlib.reload(hmm)\n",
    "\n",
    "# Now import the names you use AFTER the reload\n",
    "from corpus import TaggedCorpus  # avoid importing Sentence in helpers\n",
    "from hmm import HiddenMarkovModel\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict, Any, Tuple, Union\n",
    "from pathlib import Path\n",
    "import math, torch\n",
    "import inspect\n",
    "from corpus import TaggedCorpus, Sentence\n",
    "from hmm import HiddenMarkovModel\n",
    "\n",
    "Pathish = Union[str, Path]\n",
    "\n",
    "@dataclass\n",
    "class Phase:\n",
    "    name: str\n",
    "    corpus: Union[str, Pathish, List[Pathish], TaggedCorpus]  # \"sup\" | \"raw\" | paths | TaggedCorpus\n",
    "    weight: float = 1.0\n",
    "    tau: Optional[float] = None\n",
    "    passes: int = 1\n",
    "\n",
    "def _load_corpus(paths, tagset=None, vocab=None) -> TaggedCorpus:\n",
    "    if isinstance(paths, TaggedCorpus):\n",
    "        # already a corpus\n",
    "        return paths\n",
    "    if isinstance(paths, (str, Path)):\n",
    "        paths = [Path(paths)]\n",
    "    else:\n",
    "        paths = [Path(p) for p in paths]\n",
    "    return TaggedCorpus(*paths, tagset=tagset, vocab=vocab)\n",
    "\n",
    "def _sentence_token_count(sent: Sentence) -> int:\n",
    "    return max(0, len(sent) - 2)\n",
    "\n",
    "def _eval_cross_entropy(model: HiddenMarkovModel, eval_corpus: TaggedCorpus) -> Tuple[float, float]:\n",
    "    tot_logprob = 0.0\n",
    "    tot_tokens = 0\n",
    "    with torch.inference_mode():\n",
    "        for sent in eval_corpus:\n",
    "            if len(sent) <= 2: \n",
    "                continue\n",
    "            tot_logprob += float(model.logprob(sent, eval_corpus))\n",
    "            tot_tokens += _sentence_token_count(sent)\n",
    "    xent = -tot_logprob / max(1, tot_tokens)\n",
    "    return xent, math.exp(xent)\n",
    "\n",
    "def _strip_tags(sent):\n",
    "    n = len(sent)\n",
    "    out = []\n",
    "    for i, (w, t) in enumerate(sent):\n",
    "        out.append((w, t if i in (0, n-1) else None))\n",
    "    return type(sent)(out)\n",
    "\n",
    "\n",
    "def _eval_accuracy_breakdown(model: HiddenMarkovModel,\n",
    "                             eval_corpus: TaggedCorpus,\n",
    "                             sup_vocab: set,\n",
    "                             raw_vocab: set) -> Dict[str, float]:\n",
    "    tot = corr = 0\n",
    "    buckets = {\"known\":[0,0], \"seen\":[0,0], \"novel\":[0,0]}\n",
    "    with torch.inference_mode():\n",
    "        for sent in eval_corpus:\n",
    "            if len(sent) <= 2: continue\n",
    "            pred = model.viterbi_tagging(_strip_tags(sent), eval_corpus)\n",
    "            for j in range(1, len(sent)-1):\n",
    "                w, gold = sent[j]\n",
    "                _, ptag = pred[j]\n",
    "                tot += 1; corr += int(ptag == gold)\n",
    "                if w in sup_vocab: b=\"known\"\n",
    "                elif w in raw_vocab: b=\"seen\"\n",
    "                else: b=\"novel\"\n",
    "                buckets[b][0] += 1; buckets[b][1] += int(ptag == gold)\n",
    "    pct = lambda ok,n: 100.0*ok/n if n else float(\"nan\")\n",
    "    return {\n",
    "        \"all\": pct(corr, tot),\n",
    "        \"known\": pct(buckets[\"known\"][1], buckets[\"known\"][0]),\n",
    "        \"seen\": pct(buckets[\"seen\"][1], buckets[\"seen\"][0]),\n",
    "        \"novel\": pct(buckets[\"novel\"][1], buckets[\"novel\"][0]),\n",
    "    }\n",
    "\n",
    "def _resolve_phase_corpus(ph_corpus, model, sup_corpus, raw_corpus) -> TaggedCorpus:\n",
    "    \"\"\"Map 'sup'/'raw'/paths/TaggedCorpus → TaggedCorpus using model's tagset/vocab.\"\"\"\n",
    "    if isinstance(ph_corpus, TaggedCorpus):\n",
    "        return ph_corpus\n",
    "    if isinstance(ph_corpus, str) and ph_corpus.lower() == \"sup\":\n",
    "        return sup_corpus\n",
    "    if isinstance(ph_corpus, str) and ph_corpus.lower() == \"raw\":\n",
    "        return raw_corpus\n",
    "    # else treat as paths\n",
    "    return _load_corpus(ph_corpus, tagset=model.tagset, vocab=model.vocab)\n",
    "\n",
    "def _em_epoch(model: HiddenMarkovModel, phases: list, λ: float) -> None:\n",
    "    model._zero_counts()\n",
    "\n",
    "    # detect once whether E_step has a 'tau' parameter\n",
    "    _esig = inspect.signature(model.E_step)\n",
    "    _supports_tau = (\"tau\" in _esig.parameters)\n",
    "\n",
    "    warned = False  # warn once per epoch if tau requested but unsupported\n",
    "    for ph in phases:\n",
    "        for _ in range(ph.passes):\n",
    "            for sent in ph.corpus:\n",
    "                isent = model._integerize_sentence(sent, ph.corpus)\n",
    "\n",
    "                if _supports_tau and ph.tau is not None:\n",
    "                    model.E_step(isent, mult=ph.weight, tau=ph.tau)\n",
    "                else:\n",
    "                    if (ph.tau is not None) and (not _supports_tau) and (not warned):\n",
    "                        print(\"⚠️  tau requested but your HMM.E_step doesn't support it; \"\n",
    "                              \"running without τ-sharpening.\")\n",
    "                        warned = True\n",
    "                    model.E_step(isent, mult=ph.weight)\n",
    "\n",
    "    model.M_step(λ)\n",
    "\n",
    "def train_with_phases(\n",
    "    sup_paths,\n",
    "    raw_paths,\n",
    "    eval_path,\n",
    "    *,\n",
    "    unigram: bool = False,\n",
    "    λ: float = 0.1,\n",
    "    init_from: Optional[HiddenMarkovModel] = None,\n",
    "    phases: List[Phase],\n",
    "    max_epochs: int = 5,\n",
    "    tol: float = 1e-3,\n",
    "    verbose: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    # Build tagset/vocab from (sup ∪ raw) only\n",
    "    build_corpus = _load_corpus(list(sup_paths) + list(raw_paths))\n",
    "    model = init_from or HiddenMarkovModel(build_corpus.tagset, build_corpus.vocab, unigram=unigram)\n",
    "\n",
    "    # Canonical corpora bound to model's vocab/tagset\n",
    "    sup = _load_corpus(sup_paths, tagset=model.tagset, vocab=model.vocab)\n",
    "    raw = _load_corpus(raw_paths, tagset=model.tagset, vocab=model.vocab)\n",
    "    dev = _load_corpus(eval_path,   tagset=model.tagset, vocab=model.vocab)\n",
    "\n",
    "    # Resolve each Phase's corpus now\n",
    "    resolved_phases: List[Phase] = []\n",
    "    for ph in phases:\n",
    "        corpus_fixed = _resolve_phase_corpus(ph.corpus, model, sup, raw)\n",
    "        resolved_phases.append(Phase(ph.name, corpus_fixed, ph.weight, ph.tau, ph.passes))\n",
    "\n",
    "    # EM loop with simple dev x-ent early stopping\n",
    "    prev_xent = float(\"inf\")\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        _em_epoch(model, resolved_phases, λ=λ)\n",
    "        xent, ppl = _eval_cross_entropy(model, dev)\n",
    "        if verbose:\n",
    "            print(f\"epoch {epoch:02d}: x-ent={xent:.4f} nats, ppl={ppl:.1f}\")\n",
    "        if prev_xent - xent < tol:\n",
    "            break\n",
    "        prev_xent = xent\n",
    "\n",
    "    # Metrics\n",
    "    sup_vocab = set(sup.vocab); raw_vocab = set(raw.vocab)\n",
    "    xent, ppl = _eval_cross_entropy(model, dev)\n",
    "    accs = _eval_accuracy_breakdown(model, dev, sup_vocab, raw_vocab)\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"xent_nats\": xent,\n",
    "        \"perplexity\": ppl,\n",
    "        \"accuracy_all\": accs[\"all\"],\n",
    "        \"accuracy_known\": accs[\"known\"],\n",
    "        \"accuracy_seen\": accs[\"seen\"],\n",
    "        \"accuracy_novel\": accs[\"novel\"],\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def run_strategy_ensup_only(ensup, enraw, endev, λ=0.1, epochs=5, unigram=False):\n",
    "    return train_with_phases(\n",
    "        [ensup], [enraw], endev,\n",
    "        unigram=unigram, λ=λ, max_epochs=epochs,\n",
    "        phases=[Phase(\"sup\", \"sup\", weight=1.0, tau=None, passes=1)]\n",
    "    )\n",
    "\n",
    "def run_strategy_sup3_raw1(ensup, enraw, endev, λ=0.1, epochs=5, unigram=False):\n",
    "    return train_with_phases(\n",
    "        [ensup], [enraw], endev,\n",
    "        unigram=unigram, λ=λ, max_epochs=epochs,\n",
    "        phases=[Phase(\"sup×3\", \"sup\", weight=3.0),\n",
    "                Phase(\"raw×1\", \"raw\", weight=1.0)]\n",
    "    )\n",
    "\n",
    "def run_strategy_staged_sup_raw_sup(ensup, enraw, endev, λ=0.1, epochs=5, unigram=False):\n",
    "    return train_with_phases(\n",
    "        [ensup], [enraw], endev,\n",
    "        unigram=unigram, λ=λ, max_epochs=epochs,\n",
    "        phases=[Phase(\"pre-sup\",  \"sup\", weight=2.0),\n",
    "                Phase(\"raw\",      \"raw\", weight=1.0),\n",
    "                Phase(\"post-sup\", \"sup\", weight=2.0)]\n",
    "    )\n",
    "\n",
    "def run_strategy_tau_sharpened_raw(ensup, enraw, endev, λ=0.1, epochs=5, tau=0.7, unigram=False):\n",
    "    return train_with_phases(\n",
    "        [ensup], [enraw], endev,\n",
    "        unigram=unigram, λ=λ, max_epochs=epochs,\n",
    "        phases=[Phase(\"sup\", \"sup\", weight=2.0, tau=None),\n",
    "                Phase(\"raw-τ\", \"raw\", weight=1.0, tau=tau)]  # needs E_step(..., tau=...) support\n",
    "    )\n",
    "\n",
    "def run_strategy_enraw_only_then_tiny_sup(ensup, enraw, endev, λ=0.1, epochs=5, sup_weight=10.0, unigram=False):\n",
    "    return train_with_phases(\n",
    "        [ensup], [enraw], endev,\n",
    "        unigram=unigram, λ=λ, max_epochs=epochs,\n",
    "        phases=[Phase(\"raw-only\", \"raw\", weight=1.0),\n",
    "                Phase(\"tiny-sup\", \"sup\", weight=sup_weight)]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "358fc03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 191873 tokens from ensup, enraw\n",
      "INFO : Created 26 tag types\n",
      "INFO : Created 18461 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01: x-ent=7.2796 nats, ppl=1450.5\n",
      "epoch 02: x-ent=7.2796 nats, ppl=1450.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 191873 tokens from ensup, enraw\n",
      "INFO : Created 26 tag types\n",
      "INFO : Created 18461 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01: x-ent=7.4620 nats, ppl=1740.7\n",
      "epoch 02: x-ent=7.2455 nats, ppl=1401.8\n",
      "epoch 03: x-ent=7.2120 nats, ppl=1355.5\n",
      "epoch 04: x-ent=7.2064 nats, ppl=1348.1\n",
      "epoch 05: x-ent=7.2063 nats, ppl=1347.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 191873 tokens from ensup, enraw\n",
      "INFO : Created 26 tag types\n",
      "INFO : Created 18461 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01: x-ent=7.4184 nats, ppl=1666.4\n",
      "epoch 02: x-ent=7.2410 nats, ppl=1395.5\n",
      "epoch 03: x-ent=7.2167 nats, ppl=1361.9\n",
      "epoch 04: x-ent=7.2129 nats, ppl=1356.9\n",
      "epoch 05: x-ent=7.2129 nats, ppl=1356.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 191873 tokens from ensup, enraw\n",
      "INFO : Created 26 tag types\n",
      "INFO : Created 18461 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  tau requested but your HMM.E_step doesn't support it; running without τ-sharpening.\n",
      "epoch 01: x-ent=7.5499 nats, ppl=1900.6\n",
      "⚠️  tau requested but your HMM.E_step doesn't support it; running without τ-sharpening.\n",
      "epoch 02: x-ent=7.2689 nats, ppl=1434.9\n",
      "⚠️  tau requested but your HMM.E_step doesn't support it; running without τ-sharpening.\n",
      "epoch 03: x-ent=7.2160 nats, ppl=1361.1\n",
      "⚠️  tau requested but your HMM.E_step doesn't support it; running without τ-sharpening.\n",
      "epoch 04: x-ent=7.2065 nats, ppl=1348.2\n",
      "⚠️  tau requested but your HMM.E_step doesn't support it; running without τ-sharpening.\n",
      "epoch 05: x-ent=7.2062 nats, ppl=1347.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 191873 tokens from ensup, enraw\n",
      "INFO : Created 26 tag types\n",
      "INFO : Created 18461 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01: x-ent=7.3583 nats, ppl=1569.2\n",
      "epoch 02: x-ent=7.2699 nats, ppl=1436.5\n",
      "epoch 03: x-ent=7.2611 nats, ppl=1423.8\n",
      "epoch 04: x-ent=7.2603 nats, ppl=1422.6\n",
      "\n",
      "ensup only\n",
      "  x-ent  : 7.2796 nats  (ppl=1450.5)\n",
      "  acc all/known/seen/novel : 91.07 / 91.07 / nan / nan\n",
      "\n",
      "ensup×3 + enraw\n",
      "  x-ent  : 7.2063 nats  (ppl=1347.9)\n",
      "  acc all/known/seen/novel : 89.72 / 89.72 / nan / nan\n",
      "\n",
      "staged: sup→raw→sup\n",
      "  x-ent  : 7.2129 nats  (ppl=1356.8)\n",
      "  acc all/known/seen/novel : 89.79 / 89.79 / nan / nan\n",
      "\n",
      "τ-sharpened raw (τ=0.7)\n",
      "  x-ent  : 7.2062 nats  (ppl=1347.7)\n",
      "  acc all/known/seen/novel : 89.35 / 89.35 / nan / nan\n",
      "\n",
      "enraw-only → tiny sup\n",
      "  x-ent  : 7.2603 nats  (ppl=1422.6)\n",
      "  acc all/known/seen/novel : 90.23 / 90.23 / nan / nan\n"
     ]
    }
   ],
   "source": [
    "ensup = \"../data/ensup\"\n",
    "enraw = \"../data/enraw\"\n",
    "endev = \"../data/endev\"\n",
    "\n",
    "rows = []\n",
    "rows.append((\"ensup only\", run_strategy_ensup_only(ensup, enraw, endev)))\n",
    "rows.append((\"ensup×3 + enraw\", run_strategy_sup3_raw1(ensup, enraw, endev)))\n",
    "rows.append((\"staged: sup→raw→sup\", run_strategy_staged_sup_raw_sup(ensup, enraw, endev)))\n",
    "rows.append((\"τ-sharpened raw (τ=0.7)\", run_strategy_tau_sharpened_raw(ensup, enraw, endev, tau=0.7)))\n",
    "rows.append((\"enraw-only → tiny sup\", run_strategy_enraw_only_then_tiny_sup(ensup, enraw, endev)))\n",
    "\n",
    "for name, out in rows:\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  x-ent  : {out['xent_nats']:.4f} nats  (ppl={out['perplexity']:.1f})\")\n",
    "    print(f\"  acc all/known/seen/novel : \"\n",
    "          f\"{out['accuracy_all']:.2f} / {out['accuracy_known']:.2f} / \"\n",
    "          f\"{out['accuracy_seen']:.2f} / {out['accuracy_novel']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d33c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-class-tag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
